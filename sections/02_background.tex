\chapter{Background on Text Generation}
\label{chap:background}
Automatic text generation has been a goal of computer science researchers since the early days of computing.
In recent years, algorithm and statistical approaches have given way to neural language models--neural networks trained to build representations of human language from millions or even billions of documents.
This chapter gives a brief overview of how the task of text generation was approached before the ascendency of deep learning and then delves into the details of modern text generation using neural language models.

\section{Brief History of Text Generation}


\section{What is a Language Model?}
A language model is any model that assigns probabilities to sequences of words.
Given a sequence of words $w_1, \ldots, w_n$, a langueg model outputs the likelihood $P(w_1, \ldots, w_n)$ of this sequence.
An ideal language model would have high likelihood to natural-sounding text, like the sentences in this paragraph, and  low likelihood to gibberish.
Most language models the assumption that the likelihood of a word is dependent only on the words that precede it.
Thus, the chain rule applies:

\begin{align}
    \label{eq:lm_base}
    P(w_1, \ldots, w_n) = P(w_1) \times \ldots \times P(w_i | w_1, \ldots, w_{i-1}) \times \ldots \times P(w_n | w_1, \ldots, w_{n-1})
\end{align}
Before the transition to neural network-based models, the most common form of language model was an $n$-gram model.
Instead of trying to estimate the probability of a word given all preceding words, $n$-gram model make the Markov assumption that the probability of a word is only dependent on a fixed number of preceeding words, referred to as an $n$-grams.
For example, using a 2-gram model, we would approximate each factor in Equation \ref{eq:lm_base} as
\begin{align}
   P(w_i | w_1, \ldots, w_{i-1}) \approx P(w_i | w_{i-2}, w_{i-1})
\end{align}
An $n$-gram models can be constructed from a corpus of text by simply counting how many times each word in the text is preceeded by each possible $n$-gram.

There are several disadvantages to this $n$-gram based approach to language modelling.
First, $n$-gram models tend to be sparse.
If a particular <$n$-gram, word> pair never occurs in the corpus, then the model will assign it a probability of 0.
As a result, smoothing techniques are often employed to prevent plausible but novel word sequences from being assigned a probability of zero.
Second, the complexity of storing an $n$-gram language model grows exponentially with the choice of $n$.
In practice, most $n$-gram models used $n$ between 1 and 5, which is insufficient for modelling long-term coherence.
Third, $n$-gram models cannot represent words which are not in their vocabulary.
Such words are typically replaced with a special out-of-vocabulary identifier.
Neural language models, described in the following sections, overcome many of these limitations.


\section{What is a Neural Language Model?}

Neural network-based language models replace the statistical models described in the previous section with a learned function (the neural network) whose output can be used to predict the likelihood of a word sequence.
In contrast to $n$-gram models, neural language models are capable of assigning non-zero probability to sequences never seen in their training corpora, and thus they can be used to model sequences of hundreds or even thousands of words.

One of the key advancements in neural language modeling was the transition from operating on sequences of discrete words to operating on sequences continuous vector representations.
The sequence of words $w_1, \ldots, w_n$ is mapped to sequence of embedding vectors $\by_1, \ldots, \by_n$.
In early work on neural language modeling, these vector representations were computed separately.
Algorithms such as word2vec \citep{mikolov2013word2vec} and GloVe \citep{pennington2014glove} were employed to consturct embedding matrices where each row corresponded to a word in the chosen vocabulary.
In today's neural language models, the embedding matrix is typically treated as part of the neural language model, initialized randomly than optimized along with the reset of the network.
Let $\mathbf{E}_\theta$ be a learned embedding matrix where each row correspond to the vector represention of one word in the vocabulary.

Typical neural language models emit $\hat{\by}_t$, a predicted embedding for the $t$th position in the sequence given the previous word embeddings in the sequence. This can be written as
\begin{align}
    \hat{\by}_t = f_\theta(\by_1, \ldots, \by_{t-1})
    \label{eq:lm_feq}
\end{align}

where $f_\theta$ is the neural network and $\by_1, \ldots, \by_{t-1}$ are the embeddings of the previous tokens in the sequence.

To produce a probability distribution for what the next word should be given the previous words, the predicted embedding $\hat{\by}_t$ is multiplied by the embedding matrix $\bE_\theta$ to produce a score for each word in the vocabulary.
Then a softmax transformation is used to normalize these scores into a probability distribution.
Let $Y_t$ be a random variable representing the vocabulary item predicted for the $t$th position. We then have:
\begin{align}
    \label{equation:vocab}
    P(Y_t=i | \by_1, \ldots, \by_{t-1}) = \frac{\exp(\bE\hat{\by}_t[i])}{\sum_j\exp(\bE\hat{\by}_t[j])}
\end{align}
where $i$ and $j$ are indexes into the vocabulary.

The learned weights $\theta$ are optimized using a log likelihood loss.
% In other words, the goal is produce an $\hat{\by}_t$ that is as close to the embedding of the true next word as possible.
More precisely, we can write the training loss for a sequence $\by_1, \ldots, \by_n$ as:

\begin{align}
\mathcal{L} &= -\sum_{t=1}^n \log P(Y_t=i^* | \mathbf{y}_{1:t-1}) \\
&= -\sum_{t=1}^n \log\frac{\exp(\mathbf{E}_\theta\hat{\mathbf{y}}_t[i^*])}{\sum_j\exp(\mathbf{E}\hat{\mathbf{y}}_t[j])} \label{eq:loss_2} \\
&= -\sum_{t=1}^n \mathbf{E}_\theta\hat{\mathbf{y}}_t[i^*] \label{eq:loss_3} \\
&= -\sum_{t=1}^n (\mathbf{E}_\theta f_\theta(\by_1, \ldots, \by_{t-1})[i^*]
\end{align}

In these equations, $i^*$ is the index of the groundtruth word at position $t$ in the sequence. 
By taking the dot product between the neural network's predicted embedding and the embedding of the true word at each position $t$ (Eq. \ref{eq:loss_3}), we get a score for how correct the neural network's prediction for this position is.
Training with an objective of maximizing the sum of these scores over every word position is equivalent to minimizing the negative log likelihood (or maximizing the likelihood) of the sequence.

In some language modelling application, it is common to have an additional sequence which the model is conditioned on in addition to the tokens of the target sequence.
This paradigm is known as an encoder-decoder or sequence-to-sequence, and the formulation above is modified to
\begin{align}
    \hat{\by}_t = f_\theta(\by_1, \ldots, \by_{t-1}; \bx_1, \ldots, \bx_n)
    \label{eq:cond_lm_feq}
\end{align}
where $\bx_1, \ldots, \bx_n$ is the additional input sequence. 
The most popular application of encoder-decoder models is machine translation, where to convert some text from French to English, the language model predicts the next word of the English sequence given the entirety of the French sequence and the preceedings words of the English sequence.

Most state-of-the-art neural language models uses a variant of the Transformer architecture \cite{vaswani2017attention} as the neural network $f_\theta$.
All of the research presented in this dissertation uses the Transformer architecture, except for Chapter \ref{section:dediv}, which uses an older LSTM-based recurrent architecture \citep{hochreiter1997long}.

\section{Encoding Text into a Vocabulary}

For simplicity, the previous sections refer to the input to a language model as a sequence of words, but in practice, neural language models use a variety of different techniques to construct vocabularies of varying granularities.
% divide text into a sequence of sub-word tokens, such as those produced by a Byte Pair Encoding (BPE) \citep{sennrich2016neural}.
% Typical BPE vocabularies range from 32k to 50k words.
% Only my work in Section \TODO{decide on section} uses an older word-level vocabulary.
There is no single solution for forming the base units of language (referred to for the remainder of this chapter as ``tokens''), and techniques vary significantly across languages.
In English, the simplest vocabularies are character-level--each letter of the alphabet and punctuation becomes a token.
Historically, word-level vocabularies, where each token corresponds to a word in the dictionary, were most common.
Word-level vocabularies can be created by splitting a string on whitespace and punctuation.
Since the space of possible words is near-boundless, in practice only the most common tens or hundreds of thousands of words are included in the vocabulary, and all other words are replaced with an out-of-vocabulary token.

In recent years, subword vocabularies have become standard in neural language modeling.
Subword vocabularies are formed by choosing a budget (the desired size of the vocabulary), then running an algorithm that joins letters together into larger units, such that the most common character sequences end up as tokens in the vocabulary.
While common words such as ``cat'' or ``dog'' end up as single tokens in the vocabulary, uncommon words such as hippopotamus end up bring broken into multiple tokens.
Several greedy algorithms have been proposed to approximate optimally breaking up a text corpus into subwords, but byte-pair encoding (BPE) is currently the most popular \citep{sennrich2016neural}.
Typical subword vocabulary sizes are between 32,000 and 50,000 tokens.

Table \ref{tab:tokenization} shows the same string under a few different tokenization schemes.

\begin{table}[h]
    \centering
    \caption{Examples of the string ``A hippopotamus ate my homework." tokenized using three different vocabularies. With the subword tokenizer, the rare word ``hippopotamus" gets broken up into multiple tokens. If the word hippopotamus occurred very infrequently in the corpus used to build the vocabulary (or perhaps the writer of the sentenced misspelled it), word-level tokenization strategies would typically replace it with an out-of-vocabulary token (row 4).}
    \label{tab:tokenization}
    \begin{tabular}{l|p{5in}}
        Vocab Type & Example \\
        \hline
        character-level & \texttt{['A', ' ', 'h', 'i', 'p', 'p', 'o', 'p', 'o', 't', 'a', 'm', 'u', 's',' ', 'a', 't', 'e', ' ', 'm', 'y', ' ', 'h', 'o', 'm', 'e', 'w', 'o', 'r', 'k', '.']}\\
        subword-level & \texttt{['A', 'hip', '\#\#pop', '\#\#ota', '\#\#mus', 'ate', 'my', 'homework', '.']}\\
        word-level & \texttt{['A', 'hippopotamus', 'ate', 'my', 'homework', '.']}\\
        word-level & \texttt{['A', '[UNK]', 'ate', 'my', 'homework', '.']}\\
    \end{tabular}
\end{table}

For all of the types of vocabularies discussed, a decision must be made on whether to convert strings to lowercase before vocabulary creation.
Removing case allows for a more compact vocabulary, but it also removes potentially useful information about the location of proper nouns.

Subword vocabularies were designed to be a compromise between the advantage and disadvantages of word-level and character-level vocabularies.
Character-level vocabularies are usually very small, no more than a couple hundred tokens.
However, the vocabulary can cover near every possible string a person could write.
Word-level vocabularies cannot feasible contain the hundreds-of-thousands of words present in English text.
Realistically, only the most common words are kept, and less common ones are replaced with a special \texttt{UNK} token.
When text is tokenized with character-level vocabularies, the resulting sequences are very long, while word-level tokenization yields shorter sequences since there is just one token per word.
Lastly, word-level representations learned by a neural net tend to more meaningful than character-level representations since a word has semantics associated with it that are common across uses.
Subword vocabularies adopt the best of both worlds, using word-level tokens for common words but falling back to subword, or in the worst case, character-level, tokenization for uncommon words.
This approach eliminates the need for an out-of-vocabulary token and results in tokenized sequence lengths which are somewhere between the two strategies.


\section{Generating Text with a Language Model}

Neural language models in themselves are not generative.
As described in the previous sections, most language models provide a probability distribution for what the next token in the sequence \textit{could} be, given the previous tokens.
To perform generation, an algorithm for decoding sequences from these predicted probability distributions is needed.
At each step of decoding, the decoding algorithm performs a forward pass on the neural network using the existing prompt text as input, selects a next token based on the neural network's predictions, adds this token to the prompt, and repeats until the desired number of tokens have been generated.

There are many possible strategies for making a token selection at each step.
The simplest strategy is to take the $\arg \max$ of the distribution, greedily picking the token with the highest probability according to the model.
This approach is simple but only allows a single generation to be produced for any given prompt.
Alternatively, one can randomly sample from the vocabulary, where each vocab item is assigned a sampling probability proportional to its probability predicted by the language model.
This method allows for many different sequences to be generated from the same prompt.
However, in practice, this tactic results in noisy, low-quality text, as the probability distributions returned by neural language models tend to be very long tailed, and the chance of sampling a word from this long tail is quite high.

Several strategies have been proposed to improve random sampling techniques by reducing the entropy of the distribution before sampling.
Introducing a temperature parameter $\tau$ into the softmax computation allows us to smoothly shift probability mass from low-scoring item in the vocabulary to high-scoring ones.
\begin{align}
    \label{equation:softmax_with_temp}
    P(Y_t=i | \by_1, \ldots, \by_{t-1}) = \frac{\exp(\bE\hat{\by}_t[i]/\tau)}{\sum_j\exp(\bE\hat{\by}_t[j]/\tau)}
\end{align}
Alternatively, one can introduce sparsity intro the distribution by deliberately zeroing out like-likelihood vocabulary items.
Top-$k$ random sampling accomplishes this by restricting sampling to only the $k$ most likely tokens at each step.
Nucleus sampling, also referred to as top-$p$ random sampling, accomplishes this by restricting sampling at timestep $t$ to the $k_t$ most likely tokens, where $k_t$ is selected such that these tokens cover no more than $p$\% of the probability mass.
For all three of these techniques there is a parameter ($\tau$, $k$, or $p$) which controls the amount of randomness we want to permit in the generation.
Choosing a low value for these parameters results in an increasingly peaky distribution, which, at its extreme, is the same as taking the $\arg \max$. 
Choosing a high value for these parameters results in the distribution that looks closer and closer to the original scores produced by the model.
The diversity-quality tradeoff that results from choice of these parameters is described in detail in Chapter \ref{section:detection}.

Lastly, search methods such as beam search can be used to try and find the most likely overall sequence.
Actually computing the most likely sequence is intractable due to the exponential search space, but algorithms such as beam search so a good job of approximating it.
In practice, these techniques tend to lead to text that is bland and repetitive,
Chapter \ref{section:dediv} goes into detail about why this is the case.

\section{Methods for Controllability and Task-Specific Generation}

In the early days of neural language modelling, it was common to train a separate neural language model for each generative task of interest.
For example, if one wanted a system capable of producing chatbot dialog, one would train their neural language model on a dialog dataset such as OpenSubtitles \citep{vinyals2015neural}.
If one wanted a system able to perform text summarization, one would likewise train a model from scratch on a dataset such at Gigaword \cite{nallapati2016abstractive}.
At the time, the neural networks being used for these sorts of tasks were relatively small, and training and maintaining one model per task, was mostly feasible.

In 2018, Radford et al. \citep{radford2018improving} proposed the idea of training a single ``General Purpose Transformer (GPT)'' model which could be subsequently finetuned for any number of language tasks.
The idea of finetuning a more general model for a specific task had already taken off in computer vision, where researchers had shown a convolutional neural network trained on the ImageNet task of classifying the contents of images could be finetuned for tasks ranging from image segmentation to \TODO{}.

General purpose language models intended for generative tasks tend to be trained on massive datasets scraped from the internet, such as C4 \citep{raffel2019exploring} or The Pile \citep{pile}.
It is common to use both decoder-only models trained only to predict the next token given the previous ones \citep{radford2019language}, as well as encoder-decoder architectures trained with a de-noising loss, where the input is a corrupted version of the text, and the task is to recover the uncorrupted text \citep{raffel2019exploring,lewis2020bart}.
Finetuning such models has yielded immense success in tasks across the field of natural language processing.
Chapter \ref{section:fitb} focuses on the feasibility of finetuning for the fill-in-the-blank task.

There are however several limitations to the paradigm of pre-training followed by finetuning.
As state-of-the-art neural language models increase in number of parameters, the computational expense of finetuning is becoming increasingly prohibitive.
Furthermore, the need to store (potentially in GPU memory) one set of model weights per task makes it very difficult to build downstream applications which need to perform several different tasks.
In addition, finetuning only works where there is enough data to fine-tune one.
Overfitting is a significant challenge in low-resource settings, where there may only be a handful of training examples.

For these reasons, various approach have been proposed for replacing the finetuning step with methods which require either no or minimal weight training.
Brown et al. \citep{brown2020language} introduce the technique of few-shot prompting.
By constructing a textual prompt which contains several examplars of the goal task, a general-purpose language model can be made to perform the task.
Chapter \ref{section:style_transfer} uses this technique for the task of textual style, while Chapter \ref{section:wordcfaft} shows how it can be used for a variety of story editing operations.
Lester et al. \citep{lester2021power} introduce \textit{prompt tuning} as an improvement over few-shot prompting that trains a small neural network to produce an optimal prompt in embedding-space for the goal task.
