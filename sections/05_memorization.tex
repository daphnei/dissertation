\newcommand{\Approx}[0]{\textsc{NearDup}}
\newcommand{\Exact}[0]{\textsc{ExactSubstr}}
\newcommand{\Original}[0]{\textsc{Original}}
\sethlcolor{lightyellow}
\newcommand{\pl}[1]{\hl{#1}}

\definecolor{pink}{RGB}{255, 71, 76}
\definecolor{kelleygreen}{RGB}{0, 147, 55}
\definecolor{purple}{RGB}{102, 102, 225}
\definecolor{lightyellow}{RGB}{255, 252, 187}

\chapter{Memorization}
\label{chap:memorization}



\section{Introduction}



A key factor behind the recent progress in natural language processing is the development of large-scale text corpora used to train increasingly large language models.
These datasets have grown from single gigabytes to as much as a terabyte over the past few years \citep{chelba2013one,xue2020mt5,graff2003english,brown2020language}.
%
Because it is so expensive to perform manual review and curation on massive datasets, they tend to suffer in quality compared to their smaller predecessors.
This has implications far beyond metrics like perplexity and validation loss, as learned models reflect the biases present in their training data \cite{bender2021stochastic,wallace2019universal,sheng2020towards}.
% As a result,
Quantitatively and qualitatively understanding these datasets is therefore a research challenge in its own right \cite{dodge2021documenting}.

%Thus, even though large language models perform well under our current metrics, in large part this is due to the difficulty in creating metrics that adequately capture the nuance in language and human communication \cite{bowman2021fix}.

We show that one particular source of bias,
duplicated training examples, is pervasive:
$10\%$ of the sequences in several common NLP datasets are repeated multiple times.
%Duplicate text in training datasets has four distinct drawbacks.
While naive deduplication is straightforward
(and the datasets we consider already perform some naive form
of deduplication), performing thorough deduplication at scale is both computationally challenging and requires sophisticated techniques.

We propose two scalable techniques to detect and remove duplicated training data.
\textit{Exact} substring matching identifies verbatim strings that are repeated.
    This allows us to identify cases where only part of a training example is duplicated (\S\ref{sec:exact}).
\textit{Approximate} full document matching uses hash-based techniques~\cite{broder1997resemblance} to identify pairs of documents with high $n$-gram overlap (\S\ref{sec:approx}).

% \subsection{Consequences of Dataset Duplicates}

We identify four distinct advantages to training on datasets that have been thoroughly deduplicated.
\begin{enumerate}

\item 
Over $1\%$ of tokens emitted unprompted from a model trained on standard datasets (e.g., C4) are part of a memorized sequence (See \S\ref{sec:memorization-results})---even though the 1.5 billion parameter model is much smaller than the 350GB dataset it was trained on.
By deduplicating the training dataset we reduce the rate of emitting memorized training data by a factor of $10\times$.

\item Train-test overlap is common in non-deduplicated datasets.
For example, we find \emph{a 61-word sequence}%
\footnote{``by combining fantastic ideas, interesting arrangements, and follow the current trends in the field of that make you more inspired and give artistic touches. We'd be honored if you can apply some or all of these design in your wedding. believe me, brilliant ideas would be perfect if it can be applied in real and make the people around you amazed!''} 
in C4 \citep{t52020} that is repeated $61{,}036$ times verbatim in the training dataset and $61$ times in the validation set ($0.02\%$ of the samples in each dataset).
This train-test set overlap not only causes researchers to over-estimate model accuracy, but also biases model selection towards models and hyperparameters that intentionally overfit their training datasets.
%Models are therefore \emph{explicitly encouraged} to overfit on the (duplicate) training examples which are likely to appear in the (i.i.d. sampled) validation set as well.
%This overfitting contrasts sharply with how humans learn language \citep{linzen2020can}.

\item Training models on deduplicated datasets is more efficient.
Processing a dataset with our framework requires a CPU-only linear-time algorithm.
And so because 
these datasets are up to $19\%$ smaller, even including the deduplication runtime itself, training on deduplicated datasets directly reduces the training cost in terms of time, dollar, and the environment~\cite{bender2021stochastic, strubell2019energy, patterson2021carbon}.


\item Deduplicating training data does not hurt perplexity: models trained on deduplicated datasets have no worse perplexity compared to baseline models trained on the original datasets. 
In some cases deduplication reduces perplexity by up to $10\%$.
Further, because recent LMs are typically limited to training for just a few epochs \cite{radford2019language,t52020},
by training on higher quality data the models can reach higher accuracy faster.
\end{enumerate}
%There is therefore no reason to not perform deduplication, and every reason to perform deduplication.
To summarize, data duplication offers significant advantages and no observed disadvantages.
In the remainder of this paper we present our text deduplication framework in \S\ref{sec:methods}, and study the extent of duplicate content in common NLP datasets (e.g., C4, Wiki-40B, and LM1B) in \S\ref{sec:deduplication-results}.
We then examine the impact of deduplication on test perplexity (\S\ref{sec:perplexity-results}) and on the frequency of emitting memorized content (\S\ref{sec:memorization-results}).
Finally, we analyze to what extent perplexity on existing, released models are skewed as a result of overlap between the train and test/validation splits (\S\ref{sec:eval-existing-models}).

\section{Related Work}
\paragraph{Large language model datasets.}
While we believe our results are independent of model architecture,
we perform our analysis on Transformer-based decoder-only language models \citep{vaswani2017attention} trained for open-ended text generation.
These current state-of-the-art models are trained on internet text.
For example, the GPT-2 family of models \citet{radford2019language} is trained on WebText, a dataset of web documents highly ranked on Reddit---however this dataset was not made available publicly.
A common dataset starting point is CommonCrawl, an index of public webpages.
Among the models trained on CommonCrawl include
GPT-3 \cite{brown2020language} with the addition of book datasets,
GROVER \cite{zellers2019defending} on a restricted subset filtered to news domains called RealNews,
and T5 \cite{t52020} on a cleaned version of common crawl called C4.
Other models are trained on more curated Internet sources---for example \citet{guo2020wiki40b} used high quality processed Wikipedia text from 40 different languages to train monolingual 141.4M parameter language models.
Non-English models necessarily use different datasets; \citet{zeng2021pangualpha} for instance introduced PANGU-$\alpha$, a family of models with up to 200B parameters that were trained on a non-public corpus of cleaned and filtered Chinese-language documents from CommonCrawl and other sources.
%Lastly, the T5 family of encoder-decoder models was trained on C4, a dataset of cleaned English CommonCrawl documents.
Since many of these datasets are not public,
we deduplicate three that are: Wiki-40B, C4, and RealNews--as well as the One Billion Word Language Model Benchmark \citep{chelba2013one}, 
a smaller
dataset commonly used for evaluation.

\paragraph{Contamination of downstream tasks.}
When models are trained on datasets constructed by crawling the Internet, it is possible the model will train on the test set of downstream target tasks.
For example, \citet[\S{}4]{radford2019language} performed a post-hoc analysis to identify 8-gram overlaps between GPT-2's training set and datasets used for evaluation,
and \citet{Dodge2021-lb} analyzed C4 and found that up to 14.4\%  of test examples for various standard tasks were found verbatim (normalizing for capitalization and punctuation) in the dataset.
A more proactive approach removes contaminated data.
\citet[Appendix B]{trinh2018simple} removed documents from their CommonCrawl-based train set that overlapped substantially with the commonsense reasoning used for evaluation.
And GPT-3 \cite[\S{}5]{brown2020language} did the reverse and removed downstream evaluation examples from their training data by conservatively filtering out any train set examples with a 13-gram overlap with any evaluation example.
Up to $90\%$ of tasks were flagged as potentially contaminated.
%They found that for some tasks over 90\% of the task examples were flagged as potentially contaminated, but they noted that many detections of contamination were false-positives, and overall the identified contamination did not strongly impact model performance.
% They re-evaluated the models on cleaned datasets, and found that the performance drop was negligible despite heavy (potential) contamination. They hypothesized that either their estimation was too conservative, or that data contamination is no longer an issue in this regime, because the training set is so large that the overfitting of even their largest model (GPT-3 175B) was mild.

In our research, we do not focus on the impact of duplicate text in pretrained models on downstream benchmark tasks; instead we address how duplicate text in the LM training and validation sets impacts model perplexity and the extent to which generated text included memorized content.

\paragraph{Memorizing training data.} The privacy risks of data memorization, for example the ability to extract sensitive data such as valid phone numbers and IRC usernames, are highlighted by
\citet{carlini2020extracting}.
%
%In this paper we are more interested in the fact that memorization happens, and not the \emph{type} of data being memorized; we treat all instances of the LM generating text that closely matches the training set as problematic.
While their paper finds 604 samples that GPT-2 emitted from its training set, we show that \emph{over $1\%$} of the data most models emit is memorized training data.
% 
In computer vision, memorization of training data has been studied from various angles for both discriminative and generative models~\citep[e.g.][]{arpit2017closer,8953411,feldman2020neural,stephenson2021geometry}
% TODO add citation back in
% teterwak2021understanding}.

\paragraph{Duplicate text in training data.}
The Book Corpus \citep{zhu2015aligning}, which was used to train popular models such as BERT, has a substantial amount of exact-duplicate documents according to \citet{bandy2021addressing}.
\citet{allamanis2019adverse} shows that duplicate examples in code datasets cause worsened performance on code understanding tasks.

\section{Language Modeling Datasets}
We analyze the presence of duplicate text in four datasets of varying sizes that have been used for training natural language generation systems, producing general-purpose pre-trained models, and for language model benchmarking.
While this paper restricts itself to English datasets, we expect that non-English datasets suffer from similar issues and could likewise benefit from de-duplication.

\paragraph{Wikipedia (Wiki-40B)}
consists of multi-lingual cleaned Wikipedia text \citep{guo2020wiki40b}.
We take the English portion, which contains 2.9M Wikipedia pages with an average length of 768 BPE tokens.
The dataset creators do not indicate any deduplication was performed aside from removing redirect-pages (e.g., ``sunflower'' to ``Helianthus'').

\paragraph{One-Billion Word benchmark (LM1B)}  contains 30M sentences of news commentary \citep{chelba2013one}.
Unlike the other datasets we analyze, LM1B's examples are one sentence long rather than multi-sentence documents.
The average example length is 32 BPE tokens.
While this dataset is extremely standard for benchmarking language models, \citet[Sec 4]{radford2019language} note it has 13.2\% overlap of the test set with the train set.

\paragraph{Colossal Cleaned Common Crawl (C4)}
is made up of 360M web documents, with an average length of 486 BPE tokens \citep{t52020}.
C4 was introduced as a pre-training dataset for T5, a set of encoder-decoder models which have been widely used in fine-tuned downstream tasks.
The dataset was previously deduplicated in a more sophisticated process
than the prior two datasets.
Each paragraph was hashed and paragraphs resulting in hash collisions were removed.
This was followed by a pass that removed placeholder text, code, and prohibited words.
See \citet{dodge2021documenting} for a detailed breakdown of the source text in C4.


\paragraph{RealNews}
is a subset of the Common Crawl consisting of articles from news domains \citep{zellers2019defending}.
%RealNews was used as the training data for GROVER, a controllable language model designed to generate news articles given metadata such as the article's publisher or title.
It contains 31M documents with average length 793 BPE tokens.
RealNews was deduplicated by inserting a hash of the first 100 characters of each document into a bloom filter \citep{bloom1970space} and then excluding any document which resulted in a hash collision.
% an one already added to the dataset. TODO add back in
Like C4, examples with duplicate URLs were excluded.


\section{Methods for Identifying Duplicates}
\label{sec:methods}

The simplest technique to find duplicate examples would be to perform exact string matching between all example pairs, but as we will show, this is insufficient.
We introduce two complementary methods for performing deduplication.
First, using a suffix array \cite{manber1993suffix}, we remove duplicate substrings from the dataset if they occur verbatim in more than one example.
Second, we use MinHash \citep{broder1997resemblance}, an efficient algorithm for estimating the $n$-gram similarity between all pairs of examples in a corpus, to remove entire examples from the dataset if they have high $n$-gram overlap with any other example.

We consider a dataset $D = \{x_i\}_{i=1}^N$ as a collection of \emph{examples} $x_i$.
Each of these examples is itself a sequence of \emph{tokens}: $x_i = \left[ x_i^1, x_i^2, \cdots, x_i^{s_i} \right]$.
% In all the datasets studied here, an example corresponds to a \emph{document}, except for LM1B, in which an example corresponds to a \emph{sentence}.


\subsection{Exact Substring Duplication} \label{sec:exact}
Due to the diversity of possibilities in human language, it is rare for the same idea to be expressed identically in multiple documents unless one expression is derived from the other, or both are quoting from a shared source.
This observation motivates deduplicating exact substrings. We call our approach \Exact{}.
When two examples $x_i$ and $x_j$ share a sufficiently long substring (that is, a substring for which $x_i^{a..a+k} = x_j^{b..b+k}$), that substring is removed from one of them.
Based on statistical analyses (\S\ref{section:exact_thresh}), we select $k=50$ tokens as the minimum matching substring length.
A breakdown of the computation needed for this approach can be found in Appendix \ref{sec:suffix-implementation}.

\subsubsection{Suffix Arrays}
This exact-substring-matching criterion, while conceptually simple, is computationally prohibitive with naive (quadratic) all-pair matching.
%
To improve the efficiency, we concatenate all the examples of the entire dataset $D$ into a giant sequence $\mathcal{S}$, and construct a Suffix Array $\mathcal{A}$ of $\mathcal{S}$.
A suffix array \citep{manber1993suffix} is a representation of a suffix tree \citep{weiner1973linear} that can be constructed in linear time in $\lVert \mathcal{S} \rVert$ \citep{karkkainen2003simple}  
and enables efficient computation of many substring queries; in particular, they allow us to identify duplicated training examples in linear time.
Suffix arrays have the advantage over suffix trees in that they are 10--100$\times$
more memory efficient \cite{manber1993suffix}, requiring just 8 bytes per input token, though they are asymptotically less
efficient for some query types.
They have been used widely in NLP, such as for efficient TF-IDF computation \citep{yamamoto2001using} and document clustering \citep{hung2007new}.


The suffix array $\mathcal{A}$ for a sequence $\mathcal{S}$ is a lexicographically-ordered list of all suffixes contained in the sequence. 
%
%That is, formally, the suffix array is defined by the computation
Formally,
\[ \mathcal{A}(\mathcal{S}) = \mathop{\text{arg sort}} \text{all\_suffixes}(\mathcal{S}) \]
%
For example, the suffixes of the sequence ``banana'' are (``banana'',  ``anana'', ``nana'' ``ana'', ``na'', ``a'')
and so the suffix array is the sequence (6 4 2 1 5 3).
In practice, we construct $\mathcal{S}$ from the BPE tokenization of the text (\S\ref{sec:impact-trained-models}).

% Suffix arrays are often preferable to suffix trees because, while asymptotically less
% efficient for some types of queries, they are 10-100$\times$
% more memory efficient \cite{manber1993suffix}, requiring just 8 bytes per input token.

%% TODO FIGURE 1 GOES HERE


\subsubsection{Substring matching}

After constructing $\mathcal{A}$, it is straightforward to identify duplicated training examples.
Suppose that the sequence $s$ was repeated exactly twice in the training dataset $\mathcal{S}$ at positions $i$ and $j$,
that is, $\mathcal{S}_{i..i+|s|} = \mathcal{S}_{j..j+|s|}$.
%
Then the indices $i, j$ will occur adjacent to each other in the suffix array $\mathcal{A}$.

Finding all repeated sequences is thus a matter of linearly scanning the suffix array from
beginning to end and looking for sequences $\mathcal{A}_i, \mathcal{A}_{i+1}$ that share a common prefix of
at least some threshold length.
%
Any satisfying sequences are recorded.
%

\subsubsection{Setting a threshold of duplicates}
\label{section:exact_thresh}
One important question is how long a substring match must be before we ought to count it as a duplicate.
%
In Figure~\ref{fig:suffix-match-len}, we plot the frequency of substring matches within the four datasets we will
consider.
For each substring of length $k$, we compute the probability that there exists another sequence of length $k$ identical to this one; formally:
\[m(k) = \mathop{\text{Pr}}_{i \in [N]}\big[ \exists j \ne i : \mathcal{S}_{i..i+k} = \mathcal{S}_{j..j+k}\big].\]
We choose $50$ tokens as the threshold to be conservative:
the ``bend in the knee'' occurs at $10$ tokens, and manual inspection of
length-$25$ matches found no false positives.
We then doubled this value to have an exceptionally large margin for error.

\subsubsection{Implementation}
\paragraph{Parallel linear time construction.}
We build a parallelized linear time suffix array algorithm.
%
As a building block, we make black-box use of the SA-IS algorithm for
constructing a suffix array in linear time \citet{nong2009linear,ko2003space}.
%
Unfortunately, this algorithm is not easily parallelized directly, so
we introduce a simple divide and conquer approach to parallelizing the array construction.


We build our implementation in Rust and extend an existing suffix array library\footnote{https://github.com/BurntSushi/suffix}
with three modification.
The first two are straightforward implementation differences:
we modify the code to allow datasets larger than $4$GB,
and we remove the requirement that strings parse as valid UTF-8 sequences in favor of raw byte sequences.
Our third change is more significant: we re-implement the algorithm
so that we can stream the suffix array itself off disk.

\paragraph{Parallel partial suffix array construction.}
%
Our divide and conquer suffix array construction algorithm starts by 
partitioning the dataset into $K$ different ``splits'' with SA-IS run
over independently on each split in parallel.
%
This algorithm still requires $O(N)$ work but runs in $O(N/K)$ wall-clock time.
%
This gives us $N$ separate suffix arrays $\mathcal{A}^i$.

Given two suffix arrays $A_1$ and $A_2$ for two sequences $S_1$ and $S_2$ it's not completely trivial to construct a single suffix array $A$ for $S = S_1 \mid\mid S_2$ 
because of the boundary conditions.
Instead, we don't build the data $S = S_1 \mid\mid S_2$ but rather let $S_1' = S_1 \mid\mid S_2[upto K]$ for some $K$ greater than the longest substring match.
Then we build the arrays on $S'_1$ and $S_2$.
To merge the arrays together we can remove the items from the first array after index $|S_1|$ and merge-sort insert them into the second.

\paragraph{Parallel merge of partial suffix arrays.}
We now merge these separate arrays together into a single suffix array $\mathcal{A}$,
%
Consider the simpler case of two partial suffix arrays $B$ and $C$ that we 
would like to merge together.
%
We can achieve this by letting $i=0$ index $B$ and $j=0$ index $C$.
%
Each iteration of the algorithm then pushes $B_i$ into $\mathcal{A}$ if
$S_{B_i..} < S_{C_i}$ and $C_i$ otherwise, repeating until $i=|B|-1$ and $j=|C|-1$.
%
To generalize to $K$ splits, we need only replace the single comparison above with
a min-heap requiring $O(\log{K}) \ll 10$ work on each iteration.

Observe that in the general case this algorithm is $O(N m \log(K))$ where $N$ is the length
of the dataset, $m$ is the average length of a prefix match, and $K$ is the number of splits.
%
It is therefore incorrect to call this algorithm linear time in the general case, for ours it is.
%
Because the length of the longest match is bounded above by the length of the longest
sequence, as long as the size of the dataset is independent of the length of the 
longest sequence in the dataset, this algorithm remains efficient.

Again, we can parallelize this operation among $L$ simultaneous jobs 
(in practice we set $K=L$ as the number of threads on our machine).
%
In the $K=2$ case, job $l$ processes $i \in [jN/L, (j+1)N/L]$, choosing
the bounds of $j$ by binary searching into $C$ so that $S_{B_{i}} < S_{C_{j}} < S_{B_{j+1}}$.
%
The case where $K>2$ is identical except that we repeat this over all $K$ partial suffix arrays.


\subsubsection{Computational Analysis.}
We run our algorithm on a single VM on the cloud with $96$ cores and $768$GB of memory.
Our algorithm is efficient, for example processing the Wiki-40B training set ($3$ million
examples containing $4$GB of text) in $2.3$ minutes wall-clock time ($2.1$ CPU-hours of work).
%
The $350$GB C4 dataset takes under 12 hours (wall-clock) to build a suffix array; although we are still memory constrained and so this corresponds to $\sim 1000$ CPU-hours. 
% 
Once the suffix array has been constructed, it takes under an hour to deduplicate the C4 dataset.


Note that this algorithm still requires that the dataset itself fits in memory
(so that we can efficiently index in arbitrary positions), but we do not need to fit the entire suffix array into memory.
This is fortunate since our suffix array requires an $8\times$ space overhead.
For example, the suffix array for the
$350$GB C4 is $1.5$TB.

%

Compared to the cost of training a language model on this dataset, the additional
work required to deduplicate the training dataset is negligible.





%%%%%%%%%%%%%%%%%%%%%%%%%

\input{tables/qualitative_examples}

\subsection{Approximate Matching with MinHash} \label{sec:approx}

\subsubsection{Overview}

We also perform \emph{approximate} deduplication based on matching entire examples.
This method, which we call \Approx, is a good complement to the \emph{exact} substring matching, especially for web crawl text, as it handles the very common case of documents being identical except for interspersed templated fields (such as the last row of Table \ref{tab:qualitative_examples}).

MinHash \citep{broder1997resemblance} is an approximate matching algorithm widely used in large-scale deduplication tasks \citep{versley2012not,GABRIEL201863,gyawali2020deduplication}, including to deduplicate the training set for a large Chinese-language LM \citep{zeng2021pangualpha}.
Given two documents $x_i$ and $x_j$, the main idea is to represent each document by its respective set of $n$-grams $d_i$ and $d_j$.
We can then use hash functions to approximate the \emph{Jaccard Index} \citep{jaccard1912distribution}:
\begin{equation}
% \operatorname{Jaccard}(d_i, d_j) = s_{i, j} := \frac{|d_i \cap d_j|}{|d_i \cup d_j|}
%\operatorname{Jaccard}(d_i, d_j) = \frac{|d_i \cap d_j|}{|d_i \cup d_j|}
\operatorname{Jaccard}(d_i, d_j) = \nicefrac{|d_i \cap d_j|}{|d_i \cup d_j|}
\end{equation}
If the Jaccard Index between $d_i$ and $d_j$ is sufficiently high, it is likely that documents are approximate matches of each other.
To efficiently approximate the Jaccard index, MinHash constructs document signatures by sorting each of the $n$-grams via a hash function, and then keeping only the $k$ smallest hashed $n$-grams.
There are multiple ways to construct estimators of the Jaccard index from these kinds of signatures \citep{cohen2016min}.

In our implementation, we use 5-grams and a signature of size 9,000. The probability that two documents are considered a potential match is
\begin{equation}
\operatorname{Pr}(d_i, d_j | \operatorname{Jaccard}(d_i, d_j) = s_{i, j}) = 1 - (1 - s_{i, j}^b)^r
\end{equation}
where $b=20$ and $r=450$ are user-settable parameters to control the strength of the filter.
See Appendix~\ref{section:minhash_details} for more details.

For each pair of documents identified as a potential match, more computationally expensive similarity metrics can be employed as a subsequent filtering step.
In particular, we identify two documents as duplicates if they are matched by the MinHash algorithm and their \emph{edit similarity} is greater than 0.8. The edit similarity between token sequences $x_i$ and $x_j$ is defined as:
\begin{equation}
    \operatorname{EditSim}(x_i, x_j) = 1 - \frac{\operatorname{EditDistance}(x_i, x_j)}{\max(|x_i|, |x_j|)}
\end{equation}

\noindent To build clusters of similar documents, we construct a graph that has an edge between two documents if they are considered a match. Then, we use the method introduced in \citet{lacki2018connected} to identify  connected components.
%

\subsubsection{Implementation Details}
\label{section:minhash_details}
For our MinHash based deduplication method, documents are first space tokenized, then each consecutive 5-gram is hashed using tabulation hashing.
The set of these hashes is the signature for the document.
For each element in a document's signature, the element is hashed using $k$ other hash functions.
The minimum hashed element for each of the $k$ hash functions is stored.
These minimum hashes are then partitioned into $r$ buckets, with $b$ hashes per bucket.
These $b$ hashes are augmented into a single value, then if two documents have the same value in at least one bucket, they'll be marked as a potential match.
The probability that two documents are considered a potential match is equal to
\begin{equation}
\operatorname{Pr}(d_i, d_j | \operatorname{Jaccard}(d_i, d_j) = s_{i, j}) = 1 - (1 - s_{i, j}^b)^r
\end{equation}
where $s_{i,j}$ is the Jaccard index between the two documents.
For document pairs that were identified as potential matches, we computed their actual Jaccard index, and if that was above 0.8, we computed their edit similarity.
Document pairs with edit similarity higher than 0.8 were identified as duplicates.
After some experimentation, we chose to use $b=20$, and $r=450$, so $k=9,000$, so as to make sure a collision at the desired Jaccard index threshold of 0.8 had a high probability of occurring
% $k=9,000$, $b=20$, and $r=450.
% $k=800$, $b=20$, and $r=40$.

We also tested an alternative configuration---filtering to document pairs with Jaccard index of at least 0.9 and edit similarity of at least 0.9.
In this case, we used $b=20$, $r=40$, and $k=800$.
Figure \ref{fig:neardup-hist} shows the histogram of Jaccard similarities and edit similarities for all document pairs which collided in min-hash space, for our chosen configuration (blue) and for the alternative configuration (orange).
This allows us verify if the threshold chosen has few comparisons around the chosen threshold, then we've likely captured the majority of actual near duplicates above that threshold. To verify that yourself, look at the left hand tails of the distributions. Since both 0.8 and 0.9 begin to vanish at the same point (in spite of the fact that the two thresholds are optimized for accuracy around different thresholds), we feel comfortable saying that we're capturing the majority of actual near duplicates. 


\subsubsection{Computational Analysis}
% Our Minhash algorithm works by generating $B$ hash signatures for each document, grouping the documents by each signature to form buckets, randomly sampling $K$ documents within each bucket. Pairwise comparisons are made between the elements within each sample.

Let $N$ be the number of documents and $T$ be the maximal number of tokens in a document. Edit similarity has a worst case complexity of $T^2$, so the worst case complexity is

\begin{equation}
    O(N + b k^{2} T^{2} N) = O(N)
\end{equation}

\noindent since $b$, $k$, and $T$ are all $\ll$ $N$. The left term is the complexity of grouping by the signatures, and the right represents the pathological worst case of all documents falling into the same $B$ buckets.

The highly distributed \Approx{} implementation we employed is one used for large-scale production tasks at Google.
On the English C4 dataset, the algorithm consumed approximately 41.5 kWh of energy.
Note that our choices of $k$ and $b$ were designed to produce very high recall, and with different parameters, the algorithm could be made much more energy efficient while producing similar results.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/nd3/nd3-cluster-hist-c4.pdf}
    \caption{The distribution of near-duplicate cluster sizes from running \Approx{} on C4.}
    \label{fig:nd3-cluster-hist-c4}
\end{figure}

\section{Deduplication Results}\label{sec:deduplication-results}
We deduplicate each of the four datasets with both of our two techniques.
When text was duplicated across multiple data splits, we prioritized keeping a copy in the test or validation set and removing it from the train set.
%, so we can have comparable evaluation sets.
%
% For each dataset, we measure several properties.

% Other examples from the same split that are duplicate (e.g., irrelevant training examples).




\subsection{Amount of Text Removed}
% We find that existing text datasets scraped from the internet contain duplicated content.
% While both C4 and RealNews explicitly attempt to perform deduplication [cites], they only deduplicate \emph{exact matches} of entire training sequences. 
% This underestimates the amount of duplications in the dataset. 
With \Approx{}, we found that the web-scrape datasets contain between 3.04\% (on C4) to 13.63\% (on RealNews) near duplicates (Table \ref{tab:num_duplicates}).
Near-duplicate text is much less common in Wiki-40B, forming only 0.39\% of the train set.\footnote{Most duplicates we saw were automatically generated pages, such as the outcomes of sports games.
This shows the strength of manual curation for creating high-quality datasets.}
In C4, the majority (1.8M) of near-duplicate clusters consisted of just a single pair of examples that matched against each other, but there were 280 clusters with over 5,000 examples in them (Figure \ref{fig:nd3-cluster-hist-c4}), including one cluster of size 250,933.

On average with \Exact{}, we remove more total content than with \Approx{} (despite \Exact{} not removing any examples outright)---for example removing $7.18\%$ of the tokens in C4.
%
The exception is LM1B, where \Exact{} removes $8\times$ less data than
\Approx{}.
On investigation, we find this is due to the fact that LM1B documents are significantly shorter: $90\%$ of all documents are under 50 tokens, and so are not even candidates for potential matches even if the entire sequence matched verbatim.
%
We find that both \Approx{} and \Exact{} remove similar content---$77\%$ of the training examples that \Approx{} removes from C4 have at least one verbatim length-$50$ match found by \Exact{}.


\input{tables/dup_in_datasets}
\input{tables/top_urls_deduplicated}

\subsection{Properties of Duplicated Text}
While the authors of both RealNews and C4 explicitly attempted deduplication during dataset construction, the methods were insufficient to capture the more subtle types of duplicate text commonly found on the internet.
In C4 and Wiki-40B, we qualitatively observe that much of the text identified as near-duplicated is computer-generated.
The text is identical except for the names of places, businesses, products, dates, and so on. 
Because these examples frequently differ by just a few words at a time, deduplication strategies relying on exact string matching would fail to identify a match.
Example duplicate pairs from each dataset can be found in Table \ref{tab:qualitative_examples}.
Table \ref{tab:urls} shows the URLs had the largest proportion of examples identified by \Approx{} as near-duplicates. 
For C4, these tend to be websites that sell many similar products and thus have a large amount of templated text.
For RealNews, content aggregators seem especially common.

For RealNews and LM1B, derived from news sites, we observe that many near-duplicates occur because the same news article appears on multiple news sites with slightly different formatting.
For example, in LM1B, there is one example that starts ``\textit{MINEOLA , N.Y. - New York officials say} [...]'' and another that starts ``\textit{( AP ) - New York officials say} [...]''.
The two examples are otherwise identical.

\subsection{Train / Test Set Leakage}
\label{sec:leakage}
Both deduplication methods identify overlap between the train set and the validation set (Table \ref{tab:num_duplicates}).
For example, 4.6\% of the C4 validation set and 14.4\% of the RealNews validation set examples had an approximate duplicate in their respective training sets.
Such duplication is problematic since it could cause evaluation metrics to be unfairly inflated for models that are better at memorizing their train sets.
We evaluate the effect of this leakage on publicly released models in Section \ref{sec:eval-existing-models}.

\section{Impact on Trained Models}
\label{sec:impact-trained-models}.
We trained 1.5B parameter ``XL", decoder-only, Transformer-based language models similar to GPT-2, on C4-\Original, C4-\Approx, and C4-\Exact, respectively.
We use the T5 codebase and model architecture from \citet{t52020}, and each model was trained for about two epochs on its respective dataset.
To better understand the amount of variance in the perplexities of trained models, we also trained three different random seeds of the 110M parameter ``base" model for each of the above three datasets---for a total of nine base-sized models.

For all experiments, we used a Byte Pair Encoding (BPE) vocabulary trained on C4-\Approx{} with a budget of 50K tokens, which resulted in a vocabulary the same size as GPT-2's.
We trained with a maximum sequence length of 512 tokens (for longer documents, we randomly extracted subsequences of this length.)
Each model was trained for about two epochs.
Since both C4-\Original{} and C4-\Exact{} contain approximately 365M examples, we performed 152K steps with a batch size of 4800 (or approximately 2 epochs). 
C4-\Approx{} contains approximately 350M examples, we performed 146K steps (or approximately 2 epochs).
On a 128-core TPU v3 pod slice, XL models trained on C4-\Original{} and C4-\Exact{} took approximately 131 hours (5.5 days) to train, while the XL model trained on C4-\Approx{} took approximately 126 hours to train.
Like T5, models were trained with the Adafactor optimizer \citep{shazeer2018adafactor}. A constant learning rate of 0.01 was used for the base models and 0.001 for the XL models.

The 1.5B parameter XL models had 24 layers, each with 32 attention heads. The model embedding size was 2,048, the feed forward layers had a hidden size of 5,120, and the key/value dimension size for the attention heads 64.
The 110M parameter base models had 12 layers, each with 12 attention heads.
The model embedding size was 768, the feed forward layers had a hidden size of 2,048, and the key/value dimension size for the attention heads 64.

\subsection{Model Perplexity}\label{sec:perplexity-results}

\begin{figure}[t]
    \centering
    \begin{overpic}[width=\linewidth]{figures/eval-base-ppl_withLM1B.pdf}
    \put(1,1){\small\textbf{(a)} Base model}
    \end{overpic}\vskip5pt
    \begin{overpic}[width=\linewidth]{figures/eval-xl-ppl_withLM1B.pdf}
    \put(1,1){\small\textbf{(b)} XL model}
    \end{overpic}
    % \includegraphics[width=0.6\linewidth]{figures/eval-xl-ppl_withLM1B.pdf}
    \caption{
Impact of deduplicating the training set on validation perplexity. In \textbf{(a)}, we plot the results from T5 base (110M parameters) across three training runs with different random initializations. The black bar represent the lowest perplexity to the highest perplexity, and the colored bar the median perplexity. 
    In \textbf{(b)}, we plot the results from T5 XL (1.5B parameters).
For C4, we evaluate on \textit{C4 Original}, the original validation set; \textit{C4 Unique}, a subset of the validation set identified by \Approx{} as having zero matches across C4; and \textit{C4 Duplicates}, a subset of the validation set identified by \Approx{} as having a match in the C4 train set.
}
\label{fig:eval-ppl}
\end{figure}

We computed the perplexity of our trained models on the validation sets of LM1B and Wiki-40B, and on subsets of the C4 validation set (Figure \ref{fig:eval-ppl}).
For the base size, we observe that all models have similar perplexity on the original C4 validation set and on validation set examples that were identified as unique (no near-duplicate in either train or validation).
However, both models trained on deduplicated data have significantly higher perplexity on validation set examples that have duplicates in the training set than the model trained on the original C4. \Exact-deduplicated results in higher perplexity than \Approx-deduplicated.
These trends holds true for the XL sized model as well.
While this may suggest \Exact{} duplication results in models least overfit on the train set, note that both of these techniques have
used separate duplicate thresholds and a different choice of thresholds could change the results.

When evaluating on the validation sets of LM1B and Wiki-40B, we found that models trained on \Approx-deduplicated C4 consistently achieved lowest perplexity (for LM1B eval with base models, see Appendix Figure \ref{fig:eval-ppl-with-lm1b}).
\Exact{} deduplication decreases perplexity of the XL model by almost 3 points perplexity on Wiki-40B which is much larger than the variation of about 1 point perplexity we observed in the base models.
This is despite seeing fewer tokens of training data overall.

Lastly, we note all our XL models achieved  <35 perplexity on LM1B, which is less than the 42.16 perplexity reported for the 1.5B GPT-2 using a vocabulary the same size as ours.


\subsection{Generated Text}\label{sec:memorization-results}
Data duplication has the effect of biasing the trained LM towards particular types of examples. 
This can contribute to a lower diversity of generations, and increased likelihood that the generated content is copied from the training data \citep{carlini2020extracting}.
For our generation experiments, we use top-$k$ random sampling with $k=50$ and experiment with prompted and unprompted generation.

\input{tables/memorizations_no_prompt}
\paragraph{No prompt.}
We first evaluate memorization tendencies in the case where the model is asked to generate text without any prompt sequence.
We generate 100,000 samples, each up to 512 tokens in length (examples provided in the Appendix).
For each generated token, we say the token is memorized if it is part of a 50-token substring that is exactly contained in the training data.
On XL-\Original, over 1\% of the generated tokens belong to memorized sub-sequences (see Table~\ref{tab:memorizations_no_prompt}).
This is $\sim10\times$ more memorization than XL-\Exact{} or XL-\Approx.
Some example subsequences that were copied verbatim from the train set can be found in Table \ref{tab:exact_substr_examples} in the Appendix.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/memorized_continuations_distribution.pdf}
    \caption{Memorized continuations distribution}
    \label{fig:mem-cont-dist}
\end{figure}

\begin{figure}
    \centering
    \small
    \includegraphics[width=0.6\linewidth]{figures/memorized_continuations_fraction}
    \caption{The proportion of generations which have edit similarity above 0.8 with the groundtruth continuation when using the LM to generate continuations for 32-token prompts identified by \Approx{} as either duplicated or unique.}
    \label{fig:ground-truth-continuation}
\end{figure}
\paragraph{With prompting.}
In most real use cases, language model generation is controlled by providing a prompt for the model to continue.
We experiment with four possible prompt sources: training examples identified by \Exact{} as having near-duplicates in the train set (train dup), training examples identified as unique (train unique), validation set examples with a near-duplicate in the train set (valid in train), and validation set examples identified as unique across all splits (valid unique).
We select the first 32 tokens of each example as the prompt, which means we can evaluate the fraction of generations which are near-duplicates with the ground-truth continuation for the prompt.
Figure \ref{fig:ground-truth-continuation} shows the proportion of generations which meet this requirement, while Figure \ref{fig:mem-cont-dist} shows the distribution in edit similarities between the generations and ground-truth continuations.
When the prompt comes from duplicate examples in the train set, XL-\Original{} reproduces the groundtruth continuation over 40\% of the time.
XL-\Exact{} and XL-\Approx{} still copy the groundtruth more often when the prompt comes from a duplicate example than when the prompt comes from a unique example, suggesting that more stringent deduplication may be necessary to remove memorization tendencies entirely. 

\input{tables/ppl_on_sota_models}
\subsection{Impact on Existing Models} \label{sec:eval-existing-models}
Train-test leakage does not just impact models trained on C4.
Table \ref{tab:ppl_sota_models} shows that
% the presence of a near-duplicate
the presence of near-duplicates of the evaluation set
% whether or not an evaluation example has a near-duplicate
in the train set has a significant impact on model perplexity for two standard models: Transformer-XL \citep{dai2019transformer}, which was trained on LM1B, and GROVER \citep{zellers2019defending}, which was trained on RealNews.
For Transformer XL, the perplexity halves on examples identified as near-duplicates.
For GROVER, the difference, though not quite as stark, is present in both model sizes considered.

Existing models also suffer from the problem of generating text from their train sets.
We find that $1.38\%$ of the tokens in the official release of 25k GROVER-Mega outputs
%\footnote{\url{gs://grover-models/generation_examples/generator=mega~dataset=p0.90.jsonl}}  % TODO SPACE
are part of verbatim matches in RealNews of at least length $50$.
Likewise, more than 5\% of the tokens in \textasciitilde 200k sequences outputted by GPT-Neo 1.3B \citep{gpt-neo} are part of a $50$ token matches of its training data, the Pile \citep{pile}.

\section{Discussion}
The focus of this paper is on the datasets used to train language models.
While recent work focused on documenting the potential harms that could arise from problematic datasets  \cite{bender2018data, gebru2020datasheets}, less work has been done to 
quantitatively analyze properties of real language modelling datasets, like \citet{dodge2021documenting} has done for C4.
Our paper provides analysis on one particular axis, that of data duplication.

Our experiments measured what could be quantified: the amount of duplicate content in common datasets, the effect of deduplication on trained model perplexity, and the reduction of memorized content in trained models through deduplication.
We do not focus on the nature of the data being removed by deduplication or memorized by LMs.

Privacy is an important subject for future work, as memorized training data has significant privacy consequences.
By this, we mean the standard privacy definition that a model should not reveal anything particular to the specific dataset it was trained on, as opposed to another training dataset from a similar distribution \citep{shokri2017membership}.\footnote{%
Another interpretation of privacy focuses on the sensitivity of the data involved, when a model is trained on and able to reproduce personal identifiers or other forms of ``private data.'' Our definition is more expansive.}
Training on standard datasets that have not yet been deduplicated results in models that are particularly sensitive to examples that happened to be repeated multiple times, and this has negative privacy implications.
For instance, it could violate a person's expectations of privacy if their publicly available personal data appeared in a different, surprising context.
Downstream applications of LMs, such as the game AI Dungeon\footnote{\url{https://play.aidungeon.io/}}, should also not output memorized content like adverts for real products. 

We stress that in our experiments, we do not distinguish between undesired memorized text (such as phone numbers), innocuous memorized text (common phrases), and text we may want to be memorized (such as a quote by a public figure), and instead treat all instances of the LM generating text that closely matches the training set as problematic.
While we qualitatively observed that much of the identified memorized content was relatively innocuous, a more systematic study of the risks associated with the detected memorization was beyond the scope of this work.

We also do not investigate the negative consequences of deduplication.
Some language tasks explicitly require memorization, like document retrieval or closed-book question answering. 
Also, text that gives attribution is often duplicated across documents, so
removing duplicate substrings could correspond to removing \emph{just} the attribution, which could result in models that learn the content without its attached attribution.
Deduplication is also not sufficient to remove privacy-sensitive data like bank passwords and medical records which should never be used in training data.

Ultimately, whether memorization is a desired property of a language model, or else risky and unwanted, depends
% both % TODO add back in
on the nature of the text that has been memorized and on the downstream applications of the trained model.
However, since the trend has been towards creating datasets and models that are application-agnostic, we encourage researchers to think carefully about the limitations of the data
% they have % TODO add back in
collected and the how the model's intended usage constrains what should be part of the training set. 
% For example, if the goal of training a language model is to be able to exactly reproduce text found on the internet, then perhaps deduplication does not make sense.
% However, for the vast majority of related work, the goal is to model human communication through language, which means training on low-quality highly duplicated text undesirable.
Developing techniques to memorize or forget specific sequences depending on the end application is a promising research direction. 

\section{Conclusion}

We encourage future language model research to perform dataset deduplication, either by training on the deduplicated datasets we release, using the deduplication tools we release, or following our approach to deduplicate datasets with new tools.

The exact technique used to perform deduplication is less important than performing stringent deduplication in the first place.
On the whole, deduplication does not harm, and sometimes improves, model perplexity, despite the fact that the deduplicated datasets are smaller and faster to train on.
It is especially important that there are no duplicates between the training and testing sets, because overlap here explicitly encourages selecting models that memorize the training data.
Lastly, deduplication helps to reduce some of the privacy concerns around LMs memorizing their training data.
