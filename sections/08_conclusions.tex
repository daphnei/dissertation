\chapter{Conclusions}

\TODO{This section is still in progress.}

This thesis focuses on (1) analyzing neural language models to better understand the text they are able to generate, and (2) applications.
The main takeaways from my research are the following:

\textbf{Detection of generated text is getting harder but no less important.}
	Chapter \ref{chap:decoding}, I present the challenge of detecting machine-generated text.
	As neural language models get better, it is becoming only more challenging for humans to notice they are interacting with a bot.
	Indeed, in a recent controversary, a YouTuber inundated 4chan with GPT-Neo generated text \footnote{\url{https://youtu.be/efPrtcLdcdM}}.
	While some users eventually caught on, many continued to be fooled even after the YouTuber revealed the scheme.
	In computer vision \citep{saharia2022photorealistic,ramesh2022hierarchical}, it is standard to watermark generated images, but thus far, this has not been applied to large language model outputs.
	The increasing pervasiveness of generated text on the internet is problematic not just because of its potential societal impacts but because it sullies our future training sets.
	In machine translation, it is a well-known problem that automatically translated text could corrupt training sets, and some watermarking techniques have been proposed \citep{venugopal2011watermarking}.
	This is a problem that those who build large neural language models (and their training sets) need to start concerning themselves with as as well.
	My research on automatic detection was performed in 2018 on 768M paramter models.
	It would be very valuable to reconsider the automatic detection problem on state-of-the-art generation systems and in more realistic contexts (such as on documents where only a portion of the text may be generated).

\textbf{Text generation involves tradeoffs.}
	In Chapters \ref{chap:background} and \ref{chap:decoding}, I describe how there exists a tradeoff between generating diverse text that is easier for humans to detect because it contains obvious errors--and generating mundane text that is harder to detect, but lacks the lexical diversity of a real human writer.
	This tradeoff continues to be important, both for academic research--because we need to ensure that  comparisons between different NLG systems are fair--and for practioners--because the setting chosen can have a singificant impact on user experience.
	For example, Wordcraft users complained that the text was in too dull a style, a problem that might have been resolved very simply by increasing the sampling temperature.
	Furhter research is needed into techniques for sampling from the long tail of low-likelihood words without causing semantic errors.

\textbf{Memorization is a serious concern.}
	In Chapter \ref{chap:memorization}, I focus on the memorization problem; language models are capable of regurgitating text from their train sets.
	In many cases 
	Sometimes memorization is good (e.g., we might want our language model to be able to accurately quote famous speeches), but in other cases it is a sign of poor generalization, and at its worst, it could divulge private information.
	Important future work will be in developing techniques for allowing models to quote verbatim from data when there is a good reason to, but disallowing them otherwise.

\textbf{Supporting many tasks from fewer models is valuable.}
	\TODO{}

\textbf{Evaluation of NLG systems should happen in real-world settings.}
	\TODO{}

\TODO{More text about future directions.}