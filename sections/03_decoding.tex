% \chapter{The Diversity-Quality Tradeoff and Its Impact on Generated Text Quality and Detectability}
\chapter{Factors that Impact Generated Text Quality, Diversity, and Detectability}
\label{chap:decoding}

\section{Motivation}
It is not a question that natural language generation systems have made leaps and bounds in the last decade.
But how do we evaluate this progress and determine whether one NLG system on average produces better outputs than another NLG system?
In my research, I show how "better" is difficult to define, and I propose the task of detection--identifying whether a piece of text was written by a human or generated by a system--as a way to better understand and compare the strengths of different NLG systems. 

It turns out that choosing the choice of decoding strategy (see Section \TODO{}) has a huge impact on the quality of the text that gets generated with a neural language model.
Search-based decoding strategies which optimize for choosing the mostly likely overall sequence end up producing text which is much less diverse than text a human writer would produce.
This may be tolerable for applications where diversity isn't strictly needed, such as machine translation, but it is problematic for applications where there are many valid ways to continue a prompt.
In this chapter, I present investigations of how the choice of decoding strategy creates a trade-off between the human-assessed quality of generated text and the amount of lexical diversity present in said text.

In Section \ref{section:dediv}, we consider search-based decoding strategies, focusing especially on diversity-promoting modifications of beam search.
We compared several methods which had previously never been compared with each other, and we showed that none of these diversity-promoting methods improve diversity without serious cost to generation quality.
No method outperformed the others on both diversity and quality.

In Section \ref{section:detection}, we focus on probabalistic generation methods which randomly sample a next word using the distribution predicted by the language model.
The simplest strategy is to sample directly from the predicted distribution.
However, when many low-likelihood words cumulatively contain quite a bit of probability mass, choosing one of these words can lead to odd or contradictory phrases and semantic errors.
Humans readers are quick to notice these types of errors.
Thus, more commonly, methods are used that reduce the entropy of the distribution before sampling, which improves generation quality at the cost of diversity.
We show that humans have a hard type identifying that text is machine-generated when sampling is heavily restricted to only high-likelihood words.

Finally in Section \ref{section:roft}, we conduct a large-scale study of the detectability of generated text by human annoators, expanding upon the small-scale human evaluation experiments described in Section \ref{section:detection}.

% \section{Definition and Measurement of Diversity}
\section{Terminology}
\subsection{Diversity}
The term ``diversity'' has been used in the language model literature to refer to a diverse set of properties.
Some use it as a synonym for sentence interestingness or unlikeliness \citep{tatsunori2019unifying}.
Others consider diversity a measure of how different two or more sentences are from each other \citep{vijayakumar2016diverse,gimpel2013systematic}.
In some framings, diversity is measured across a set of generations coming from the same prompt.
Given a particular prompt or input, the goal is to measure the breadth of possible generations the model will produce \citep{mayhew2020simultaneous}.
Diversity can also be measured as a corpus-level: given all the sentences generated by the model for all prompts, what is the overall lexical diversity \TODO{citation}?

In Section \ref{section:dediv}, we define diversity as the ability of a generative method to create a set of possible outputs that are each valid given a particular input but vary as widely as possible in terms of word choice, topic, and meaning.
We also use human evaluation to measure generation interestingness.
In Section \ref{section:detection}, we consider decoder-only language models, where no additional input is conditioned on.
In this setting, we instead consider corpus-level diversity across all the model's generations.
In both sections, diversity of a set of model generations is automatically measured using distinct-$n$, the number of unique $n$-grams within the set divided by the total number of $n$-grams.

\subsection{Quality}
``Quality'' is also a difficult property to define.
When measured in downstream applications, it can be quantified as how many times a user interactions with the generative system (for example, the number of conversation turns with a dialog system) before losing interest.
It can also be evaluated directly by asking a human rater, "how good is this text?", though definitions of ``good'' vary widely across the literature \TODO{add citation}.

To some extent, quality can also be measured automatically.
In tasks with a clear goal, like machine translation or summarization, one can compare the generation against a gold standard.
Generally, quality is strongly associated with fluency, and in \TODO{citation} we show that up until a point, the lower perplexity text is assigned by a language model, the more fluent it is.

Sections \ref{section:dediv} and \ref{section:detection} take quite different approaches to measuring generation quality.
In \ref{section:dediv}, we consider a dialog model, and ask humans to assess the quality of model responses on three axes: fluency, adequacy, and interestingness.
In  \ref{section:detection}, we propose a novel method for assessing generation quality based on the premise that humans (or a trained discriminator) ought to have a hard time distinguishing between real human-written text and model outputs when the model outputs text that is high-quality.

\section{Diversity-Promoting Search-Based Decoding Methods}
\label{section:dediv}
\subsection{Introduction}
In 2018, state-of-the-art neural language models were based on recurrent neural networks, such as LSTMs, that struggled to generate sequences that were both long and high-quality.
However, these models were beginning to have huge success over statistical techniques in natural language processing tasks such as machine translation \cite{sutskever2014sequence,Luong2015EffectiveAT}, text summarization \cite{Nallapati2016AbstractiveTS}, and dialog systems \cite{vinyals2015neural}.

At the time, the standard convention for generation was to try to generate the most likely overall sequence from the language model.
This approach made a lot of sense in machine translation, where generating one correct translation was more important than generating diverse translation.
Since computing the overall most likely output sequence is intractable, early work in neural machine translation found that beam search was an effective strategy to heuristically sample sufficiently likely sequences from these probabilistic models \cite{sutskever2014sequence}.
However, as neural language models came to be applied increasingly to open-ended tasks, beam search was found to be ill-suited to generating a set of diverse candidate sequences; this is because candidates outputted from a large-scale beam search often only differ by punctuation and minor morphological variations \cite{li2016mutual}.

There are a number of reasons why it is desirable to produce a set of diverse candidate outputs for a given input.
For example, in collaborative story generation, the system makes suggestions to a user for what they should write next \cite{clark2018creative}.
In these settings, it would be beneficial to show the user multiple different ways to continue their story.
In image captioning, any one sentence-long caption is probably missing some information about the image.
\citet{krause2017hierarchical} show how a set of diverse sentence-length image captions can be transformed into an entire paragraph about the image.
Lastly, in applications that involve reranking candidate sequences, the reranking algorithms are more effective when the input sequences are diverse.
Reranking diverse candidates has been shown to improve results in both open dialog and machine translation \cite{li2016diversity,li2016mutual,gimpel2013systematic}. 
Furthermore, in open-ended dialog, the use of reranking to personalize a model's responses for each user is a promising research direction \cite{Choudhary2017DomainAN}.

With these sorts of applications in mind, a variety of alternatives and extensions to beam search were proposed which sought to produce a set of diverse candidate responses instead of a single high likelihood one \cite{li2016diversity,vijayakumar2016diverse,kulikov2018importance,tam2019clustered}.
Many of these approaches show marked improvement in diversity over standard beam search across a variety of generative tasks.
However, as of 2018, there had been little attempt to compare and evaluate these strategies against each other on any single task.

In this sub-chapter, we survey methods for promoting diversity during decoding in order to systematically investigate the relationship between diversity and perceived quality of output sequences.
We focus on conditional language models--models that are trained to map from some input, perhaps an image or some text, to a target sequence.
In addition to standard beam search and greedy random sampling, we compare several recently proposed modifications to both methods.
We present a detailed comparison of existing diverse decoding strategies on two tasks: open-ended dialog and image captioning, and recommendations for a diverse decoding strategy.

\subsection{Diverse Decoding Strategies}
There are many ways to decode text from a conditional language model.
Let $\by$ represent the sequence of tokens for the target sequence ($\by = y_1 \ldots y_n$) and $\bx$ represent the input (which may be another token sequence, or, in the case of image captioning, an image).
Most decoding strategies strive to find the most likely overall sequence, i.e. pick a $\mathbf{\hat{y}}$ such that:\footnote{The formulation we are using here is slightly different than Section \TODO{}, in that we use $\by$ and $\hat{\by}$ to refer to sequence of tokens, rather than framing in terms of a sequence of embedding vectors.}
\[
    \mathbf{\hat{y}} = \arg\max_{\mathbf{y}}{ P(\mathbf{y} | \mathbf{x})} = \arg\max_{\mathbf{y}}{\prod_{t=1}^{N} {P(y_t \mid y_{<t}, \mathbf{x})}}
\]
In practice, this can't be computed exactly, no sub-exponential algorithm exists for find the optimal decoded sequence, and thus the field instead use approximations.

Beam search approximates finding the most likely sequence by performing breadth-first search over a restricted search space.
At every step of decoding, the method keeps track of $b$ partial hypotheses.
The next set of partial hypotheses are chosen by expanding every path from the existing set of $b$ hypotheses, and then choosing the $b$ with the highest scores.
If the goal is to approximate finding the most likely over-all sequence, the log-likelihood of the partial sequence is used as the scoring function.
Algorithm \ref{alg:beam-search-inference} gives an overview of the beam search algorithm. 

\begin{algorithm}
\caption{Beam Search Inference}
\label{alg:beam-search-inference}

\begin{algorithmic}[1]
\Procedure{Beam Search}{}
\State $B \gets \{SOS\}$
\State $k \gets $ BeamWidth
\State $out \gets k$-best output list
\While{$|out| < k$}
    \State $front \gets \text{remove all nodes from} B$
    \For{$w \in front$}
    \State $succ \gets w$'s $k$-best successors
    \For{$s \in succ$}
    \If{$s == EOS$}
        \State $out \gets out \cup \{s\}$
    \Else
        \State $B \gets B \cup \{s\}$
    \EndIf
    \EndFor
    \EndFor
    \State Sort $B$
    \If{$|B| > k$}
        \State Prune $B$ to $k$-best successors
    \EndIf
\EndWhile

\Return out
\EndProcedure
\end{algorithmic}
\end{algorithm}

Since beam search only explores a limited portion of the overall search space, it tends to yield multiple variants of the same high-likelihood sequence, sequences that often only differ in punctuation and minor morphological changes \cite{li2016mutual}.  
Therefore, standard beam search is not ideal for producing diverse outputs.
In this section, we will discuss a variety of methods that have been developed recently to eliminate redundancy during decoding and generate a wider range of candidate outputs.

\paragraph{Noisy Parallel Approximate Decoding}\quad
Introduced by \citet{cho2016noisy}, NPAD is a technique than can be applied to any decoding setting.
The main idea is that diversity can be achieved more naturally by taking advantage of the continuous manifold on which neural nets embed language.
Instead of encouraging diversity by manipulating the probabilities outputted from the model, diverse outputs are instead produced by adding small amounts of noise to the hidden state of the decoder at each step.
The noise is randomly sampled from a normal distribution. The variance is gradually annealed from a starting $\sigma_0$ to 0 as decoding progresses (that is $\sigma_t = \frac{\sigma_0}{t}$) under the reasoning that uncertainty is greatest at the beginning of decoding.
NPAD can be used in conjunction with any decoding strategy; following the best results from the original paper, we show results using NPAD with beam search.

Extensions to NPAD have sought to learn the direction in which to manipulate the hidden states using an arbitrary decoding objective \cite{gu2017trainable}.
Since such objectives can be highly domain-specific, we do not evaluate this method.

\paragraph{Top-$g$ Capping}\quad
In beam search, it is often the case that one hypothesis $h$ is assigned a much higher probability than all other hypotheses, causing all hypotheses in the next step to have $h$ as their parent. Following \citet{li2016mutual} and \citet{li2016simple}, we add an additional constraint to standard beam search to encourage the model to choose options from diverse candidates.
At each step $t$, current hypotheses are grouped according to the parental hypothesis they come from.
After grouping candidates, only the top $g$ from each grouping are considered. The resulting $b \times g$ candidates are ranked, and the top $b$ are selected as hypotheses for the next beam step.

\paragraph{Hamming Diversity Reward}\quad
\citet{vijayakumar2016diverse} proposes adding an additional diversity-promoting term, $\theta$, to the log-likelihood before reranking.
This term measures how different a candidate hypothesis $c^{(i)}_{\leq t}$ is from the partial hypotheses selected in the previous step. Let $\mathcal{H}_{t-1} = \{c^{(1)}_{\leq t-1}$, \ldots $c^{(b)}_{\leq t-1}\}$ be these partial hypotheses.
Then the beam search scoring function for the $i$th candidate at timestep $t$ becomes:
\begin{align*}
    \text{score}(c^{(i)}_{\leq t}) = \sum_{j=1}^t \big(\log P(c^{(i)}_j | c^{(i)}_{<j}, \textbf{x})\big) \\+ \lambda\theta(c^{(i)}_{\leq t}, \mathcal{H}_{t-1})
\end{align*}
where $\lambda$ is a tunable hyperparameter. \citet{vijayakumar2016diverse} try a variety of definitions for $\theta$, including embedding diversity and $n$-gram diversity, but they find that Hamming distance, the number of tokens in the candidate sequence which exist in the previously selected partial hypotheses, is most effective. We take the negative of the Hamming distance as $\theta$.

\paragraph{Iterative Beam Search}\quad
In an attempt to improve the size of the search space explored without sacrificing runtime, \citet{kulikov2018importance} propose an iterative beam search method.
Beam search is run many times, where the states explored by subsequent beam searches are restricted based on the intermediate states explored by previous iterations.
Formally, we can define the set of all partial hypotheses for beam search instance $i$ at time step $t$ as $\mathcal{H}_t^{(i)}$. From here, the search space explored by beam search instance $i$ can be expressed as $S_i = \cup_{t=1}^T \mathcal{H}_t^{(i)}$.
The $i$th beam search is prevented from generating any partial hypothesis that has previously been generated, that is, any hypothesis found in $S_{<i} = \cup_{i^{\prime}=0}^{i-1}S_{i^{\prime}}$.

The authors also attempt a soft inclusion criterion, where any states within $\epsilon$ Hamming distance from a previously explored state are also excluded. During the experimentation of \citet{kulikov2018importance}, however, the soft-inclusion was found to not be beneficial; thus, we only restrict exact matches of previous states in our implementation.
In practice, this means after the first beam search instance runs as normal, the first step of the second beam search instance will contain the $b$+1 to 2$b$-most likely starting tokens; this pattern holds for the third beam search instance, and so on.

\paragraph{Clustered Beam Search}\quad
Most recently, \citet{tam2019clustered} proposed a clustering-based beam search method to help condense and remove meaningless responses from chatbots.
Specifically, at each decoding step $t$, this method initially considers the top $2*b$ candidates. From there, each candidate sequence is embedded\footnote{We follow \citet{tam2019clustered} and used averaged GloVe word embeddings \cite{pennington2014glove}.}, and the embeddings are clustered into $c$ clusters using $K$-means. Finally, we take the top $\frac{b}{c}$ candidates from each cluster. Note that in the case any clusters have size less than $\frac{b}{c}$, we then include the highest-ranked candidates not found after clustering.

\paragraph{Clustering Post-Decoding (PDC)}
All the previous methods modified the decoding algorithm to encourage diversity.
However, it is also possible to encourage additional diversity post-hoc by sampling several generations and then choosing the most diverse one.
On the task of sentence simplification, after decoding using a large-scale diversity-promoting beam search (beam size 100), \citet{kriz2019complexity} then clustered similar sentences together to further increase the variety of simplifications from which to choose.
Document embeddings generated via Paragraph Vector \cite{Le2014distributed} were used as the sentence embeddings with which to perform $K$-means. 

In this work, we extend this post-decoding clustering idea in three key ways.
First, we make use of sentence-level embeddings which leverage the pre-trained language representations from the Bidirectional Encoder Representations from Transformers (BERT) \cite{devlin2018bert}.\footnote{BERT sentence-level embeddings were obtained using https://github.com/hanxiao/bert-as-service.}
Second, after clustering, \citet{kriz2019complexity} took the sentence closest to the centroid of each cluster as the representative candidate; we instead choose the highest-ranked candidate (according to log-likelihood) from each cluster to ensure the best candidates are still selected.
Finally, after performing standard $K$-means clustering, we found that it was often the case that some clusters contained large numbers of good candidates, while others contained very few candidates that are also either ungrammatical or otherwise inferior.
Thus, in our implementation, we remove clusters containing two or fewer sentences, and then sample a second candidate from each of the remaining clusters, prioritizing selecting candidates from larger clusters first.


\subsection{Experimental Setup}
We evaluate the decoding strategies described in the previous section under the following settings.
For each of the published beam search algorithms, we choose the hyperparameters that were found to be best in the original publications.\\

\small
\begin{tabular}{l|l}
    \toprule
    Method & Setting\\
    \midrule
    \multirow{2}{*}{RS} & Random sampling with temp = 0.5,\\
    & 0.7, 1.0, or 1.0 with top-10 capping.\\
    Standard BS & Standard beam search\\
    Top5Cap BS & Top-$g$ capping with $g=3$\\
    Iter5 BS & Iterative beam search with 5 iterations\\
    HamDiv0.8 BS & Hamming Diversity with $\lambda=0.8$\\
    Cluster5 BS & Clustered beam search with 5 clusters\\
    NPAD0.3 BS & Noisy Decoding with $\sigma_0=0.3$ \\
    \bottomrule
\end{tabular}
\normalsize
\\

For random sampling, we sample 10 outputs, and with beam-search based methods, we use a beam size of 10 to generate 10 outputs.
In addition, we show results from oversampling then filtering.
We use a beam size of 100 or generate 100 samples through random sampling, and then we select 10 from the 100, either through post-decoding clustering (PDC) or by taking the 10 candidates with highest likelihood. 

We examine these decoding strategies on two tasks: open ended dialog and image captioning.
For each task, we evaluate both the quality and diversity of the 10 outputs from each strategy.

\subsubsection{Open-ended Dialog Task}
In the dialog domain, we use an LSTM-based sequence-to-sequence (Seq2Seq) model implemented in the OpenNMT framework \cite{opennmt}.
We match the model architecture and training data of \citet{baheti2018generating}.
The Seq2Seq model has four layers each in the encoder and decoder, with hidden size 1000, and was trained on a cleaned version of OpenSubtitles \cite{tiedemann2009news} to predict the next utterance given the previous one.

Evaluation is performed on 100 prompts from the Cornell Movie Dialog Corpus~\cite{danescu2011chameleons}.
These prompts are a subset of the 1000 prompts used in \citet{baheti2018generating}, which were filtered using item response theory for discriminative power.

We report perplexity (PpL), averaged over \textit{all} the top 10 outputs for each example.\footnote{This differs from existing work which computes perplexity over only the top output for each example. For our task we are interested in the quality of all of the generated responses.} Since the quality of open-ended dialog is notoriously difficult to evaluate automatically, we ran a human evaluation task on Amazon Mechanical Turk where annotators were shown a prompt and 5 potential responses generated by any of our decoding methods.
Evaluators were asked to provide binary ratings on fluency, adequacy, and interestingness for each response. Overall, we collected 3 human judgments for each of the top ten responses for each of our decoding methods; in other words, we collected 3,000 judgments per method.
Figure \ref{figure:mturk_example} shows the instructions given to the human raters.

\begin{figure}
    \includegraphics[width=15cm]{figures/dediv_mturk_example}
    \caption{The full instructions for our Amazon Mechanical Turk task to evaluate the quality of our dialog system responses.}
    \label{figure:mturk_example}
\end{figure}

\begin{table}[t]
    \centering
    \tiny
    \setlength\tabcolsep{4pt} 
    \begin{tabular}{|lr||ccc||ccccc|} \hline
    
    \multicolumn{2}{|c||}{\textbf{Method}} & \textbf{Fluency} & \textbf{Adequacy} & \textbf{Interestingness} & \textbf{Ppl} & \textbf{Dist-1} & \textbf{Dist-2} & \textbf{Ent-2} & \textbf{Ent-4} \\ \hline\hline
    Reference & & 0.795 & 0.732 & 0.636 & -- & -- & -- & -- & -- \\ \hline\hline
    RS 0.7 &(sample 10) & \textbf{0.758} & 0.399 & \textbf{0.388} & 35.98 & 0.63 & 0.80 & 4.08 & 3.84 \\
    RS 1.0 &(sample10) & 0.550 & 0.303 & \td{0.386} & 67.99 & \textbf{0.74} & \textbf{0.87} & \textbf{4.35} & \textbf{4.08} \\
    RS 1.0,top10 &(sample 10) & \td{0.745} & \textbf{0.418} & \td{0.387} & \textbf{10.33} & 0.60 & 0.80 & 4.12 & 3.91 \\ \hline\hline
    Standard BS &(10 beams) & \textbf{0.950} & \textbf{0.621} & 0.336 & \textbf{4.01} & 0.37 & 0.45 & 3.16 & 3.01 \\
    Top3Cap BS &(10 beams)& \td{0.942} & 0.603 & 0.346 & 4.03 & 0.37 & 0.46 & 3.17 & 3.03 \\
    Iter5 BS &(10 beams)& 0.903 & 0.520 & 0.335 & 5.42 & \textbf{0.62} & \textbf{0.74} & \textbf{3.68} & \textbf{3.25} \\
    HamDiv0.8 BS &(10 beams)& 0.923 & 0.599 & \td{0.366} & 4.56 & 0.33 & 0.37 & 3.08 & 3.00 \\
    Cluster5 BS &(10 beams)& 0.936 & 0.582 & \textbf{0.381} & 4.23 & 0.39 & 0.46 & 3.24 & 3.06 \\
    NPAD0.3 BS &(10 beams) & \td{0.942} & \td{0.604} & 0.335 & 4.05 & 0.36 & 0.44 & 3.13 & 2.99 \\ \hline\hline
    RS 1.0,top10 &(sample 100, rank) & \textbf{0.922} & \textbf{0.548} & 0.347 & \textbf{5.10} & 0.52 & 0.68 & 3.54 & 3.18 \\
    RS 1.0,top10 &(sample 100, PDC) & 0.852 & 0.494 & \textbf{0.372} & 6.96 & \textbf{0.63} & \textbf{0.76} & \textbf{3.74} & \textbf{3.27} \\ \hline\hline
    Standard BS &(100 beams, rank) & \textbf{0.964} & \textbf{0.611} & \td{0.332} & \textbf{4.01} & 0.44 & 0.61 & 3.33 & 3.05 \\
    Standard BS &(100 beams, PDC) & 0.944 & 0.599 & \textbf{0.346} & 4.42 & \textbf{0.57} & \textbf{0.70} & \textbf{3.59} & \textbf{3.21} \\ \hline
    \end{tabular}

    \caption{
    Results on 100 dialog prompts.
    The first row shows the mean human ratings of the single reference response available for each prompt.
    The next three rows show results for random sampling, with 10 samples drawn per prompt. The next six rows are variants of beam search using beam size 10.
    The last four rows use random sampling or standard beam search to generate 100 outputs, then filter down to 10 outputs either through ranking by log-likelihood or by performing post-decoding clustering (PDC).
    In each section, the highest value is bolded, and statistical ties are marked \textdagger.}
    \label{tab:results_no_cluster}
\end{table}

\begin{table}
    \small
    \centering
\begin{tabular}{|lr||ccc||cccc|}
\hline
 & & \multicolumn{3}{c||}{\textbf{SPICE}} & & & & \\
\multicolumn{2}{|c||}{\textbf{Method}} & \textbf{Mean} & \textbf{@1} & \textbf{@10} & \textbf{Dist-1} & \textbf{Dist-2} & \textbf{Ent-2} & \textbf{Ent-4}  \\
\hline\hline
RS 0.7 &(sample10)               & \textbf{0.170} & \textbf{0.192} & \textbf{0.278} & 0.31 & 0.52 & 3.67 & 4.00 \\
RS 1.0 &(sample10)               & 0.133 & 0.167 & 0.247 & \textbf{0.44} & \textbf{0.71} & \textbf{4.17} & \textbf{4.26} \\
RS 1.0,top10 &(sample10)         & 0.159 & 0.183 & 0.272 & 0.33 & 0.59 & 3.90 & 4.17 \\
\hline \hline
Standard BS &(10 beams)               & 0.194 & 0.193 & 0.283 & 0.18 & 0.26 & 2.94 & 3.18 \\
Top3Cap BS &(10 beams)                & \textbf{0.195} & \textbf{0.196} & 0.282 & 0.17 & 0.26 & 2.93 & 3.17 \\
HamDiv0.8 BS &(10 beams)              & 0.194 & 0.194 & 0.282 & 0.18 & 0.27 & 2.98 & 3.19 \\
Cluster5 BS &(10 beams)               & 0.191 & 0.194 & \textbf{0.285} & \textbf{0.19} & \textbf{0.28} & \textbf{3.04} & \textbf{3.25} \\
NPAD0.3 BS &(10 beams)                & 0.191 & 0.192 & 0.280 & 0.18 & 0.26 & 2.94 & 3.17 \\
\hline \hline
RS 1.0,top10 &(sample100, rank)  & \textbf{0.182} & \textbf{0.188} & \textbf{0.284} & 0.25 & 0.41 & 3.31 & 3.64 \\
RS 1.0,top10 &(sample100, PDC)   & 0.169 & \textbf{0.188} & 0.282 & \textbf{0.31} & \textbf{0.52} & \textbf{3.62} & \textbf{3.91} \\
\hline \hline
Standard BS &(100 beams, rank)        & \textbf{0.188} & 0.190 & 0.279 & 0.20 & 0.31 & 3.04 & 3.32 \\
Standard BS &(100 beams, PDC)         & 0.186 & \textbf{0.192} & \textbf{0.288} & \textbf{0.24} & \textbf{0.38} & \textbf{3.25} & \textbf{3.57} \\
\hline
\end{tabular}
\caption{Image captioning results for selected random sampling and beam search methods. SPICE@1 measures the SPICE score of the most likely caption. SPICE@10 is the maximum score across the 10 candidates generated by each method. Mean SPICE is the mean score over all 10 candidates. In each section, the best value is bolded.}
\label{tab:image_captioning}
\end{table}

\subsubsection{Image Captioning Task}
For image captioning, we use a state-of-the-art model introduced in \citet{anderson2018bottom}.
We take advantage of \citet{Luo2017}'s open-source implementation and released model parameters trained on MSCOCO \cite{lin2014microsoft}.
We evaluate on a test set containing 5000 images.

We report Semantic Propositional Image Caption Evaluation (SPICE) scores, an automatic evaluation metric that has been shown to correlate well with human judgments of quality\citep{Anderson2016SPICE}. 
SPICE measures how well the semantic scene graph induced by the proposed caption matches one induced by the ground truth.
In addition to computing SPICE on the top-scoring caption (SPICE@1), we follow \citet{vijayakumar2016diverse} in reporting Oracle SPICE@10 scores.
This is done to show the upper bound on the potential impact diversity can have.
We also compute the mean SPICE score across all of the candidate captions for an image.
Unlike SPICE@1 and SPICE@10, this metric shows the overall quality of \textit{all}
of the candidate captions, which is useful to know for applications that combine diverse candidate output sequences \cite{krause2017hierarchical}.

\subsubsection{Evaluating Diversity}
To measure the diversity across the generated candidate sequences for a given input, we report \textbf{Dist-k}, the total number of distinct k-grams divided by the total number of produced tokens in all of the candidate responses for a prompt \citep{li2016diversity}. 
We report Dist-2 and Dist-4 averaged over the prompts in the test set.

A limitation of Dist-$k$ is that all $k$-grams that appear at least once are weighted the same, ignoring the fact that infrequent $k$-grams contribute more to diversity than frequent ones. 
\citet{zhang2018generating} instead propose an entropy metric, \textbf{Ent-k}, defined as:
 \begin{align*}
 \textit{Ent-k} = \frac{-1} {\sum_{w \in S}F(w)} \sum_{w \in S} F(w) \log \frac{F(w)} {\sum_{w' \in S} F(w')}
 \end{align*}
 where $S$ is the set of all $k$-grams that appear in candidate responses for an example, and $F(w)$ denotes the frequency of $w$ in the candidate responses.

\subsection{Results}
We report results on dialog systems and image captioning in Tables \ref{tab:results_no_cluster} and \ref{tab:image_captioning}, respectively. As expected, random sampling-based approaches yield outputs with greater diversity but worse quality than beam search-based approaches.
Over-sampling then filtering increases the quality of outputs while still ensuring high diversity. 
In the following sections, we discuss the diversity-quality tradeoff, and then delve further into the results for each method group.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[l]{5cm}
        \includegraphics[width=5cm]{figures/dist2_vs_human.pdf}
    \end{subfigure}
    \begin{subfigure}[l]{5cm}
        \includegraphics[width=5cm]{figures/ent4_vs_human.pdf}
    \end{subfigure}
    \begin{subfigure}[l]{5cm}
        \includegraphics[width=5cm]{figures/perplexity_vs_human.pdf}
    \end{subfigure}
    \caption{Each decoding strategy is plotted, showing that human-perceived quality is negatively correlated with diversity. The Pearson Correlation coefficients between each statistic and the average of fluency, coherence, and interestingness are shown in parentheses.}
    \label{fig:correlations}
\end{figure}

\begin{table}[t]
    \centering
   ofsmall
    \begin{tabular}{|l|l|}
    \hline
    \multicolumn{2}{|l|}{\rule{0pt}{0.35cm} \textbf{Prompt:} Look, nobody knows we did it.}  \\ 
    \hline \hline
    \textbf{RS 0.5} &  \textbf{Standard BS} \\
    \hline
\makecell[l]{
I don't know what you're talking about. \\
What's the matter with you? \\
I don't know what it is. \\
I don't think so. \\
He's got to get out of here. \\
}&
\makecell[l]{
We've got to get out of here. \\
What do you mean? \\
I don't think it's a good idea. \\
I don't know what to say. \\
I don't know what's going on. \\
}\\
    \hline
    \hline
    \textbf{RS 1.0} & \textbf{Standard BS with PDC} \\
    \hline
    
\makecell[l]{
I can't find it. \\
They're our ships. \\
It's all right anyone is the right to interfere. \\
We didn't have a plan I engineered a policy. \\
Same time you pick us up at six and get we. \\
}&
\makecell[l]{
I don't know! \\
I don't think so. \\
What do you mean? \\
Why didn't you tell me? \\
That's why we're here. \\
}\\
    \hline
    \hline
    \textbf{RS 1.0,top10} & \textbf{RS 1.0,top10 with PDC} \\
    \hline
\makecell[l]{
I don't know what else to do. \\
It doesn't have to be that way! \\
We're in the air! \\
I've seen a guy in his place in a it. \\
And I'm not we any more. \\
}&
\makecell[l]{
What do you mean? \\
I don't think so. \\
That's why I'm here. \\
It's all right we. \\
We've been through this before. \\
}\\
\hline
\hline


    \textbf{NPAD0.3 BS} & \textbf{Cluster5 BS} \\
    \hline
\makecell[l]{
I don't think it's a good idea. \\
I don't know what to say. \\
I don't know what's going on. \\
I don't know what to do. \\
I don't know what's going on here. \\
}&
\makecell[l]{
I don't know why. \\
What do you mean? \\
I don't think so. \\
How do you know that? \\
I'll tell you what. \\
}\\
\hline
\hline

\textbf{Top3Cap BS} & \\
\hline
\makecell[l]{
We've got to get out of here. \\
What do you mean? \\
I don't think it's a good idea. \\
I don't know what to say. \\
I don't know what's going on. \\
} & \\

\hline
    \end{tabular}
    \caption{Responses to an example prompt for selected methods. More examples can be seen in the appendix.}
    \label{examples}
\end{table}

\subsubsection{The Quality Diversity Tradeoff}

The goal of diverse decoding strategies is to generate high-quality candidate sequences which span as much of the space of valid outputs as possible. 
However, we find there to be a marked trade-off between diversity and quality.
This can be seen in Figure \ref{fig:correlations}, where we plot the human-judged quality score for each dialog experiment against our primary diversity descriptive statistics.
Fluency and adequacy are both strongly negatively correlated with diversity.
While we had expected interestingness to be positively correlated with diversity, the fact that it is not suggests that existing diversity statistics are insufficient for capturing what it means to humans for outcomes to be interesting.

Likewise, in image captioning, the mean SPICE score of the 10 candidate captions (averaged over all examples for each experimental setting) is strongly anti-correlated with diversity, with a Pearson correlation coefficient of -0.83 with the Ent-4 measure and -0.84 with Dist-2.
Clearly it remains an open challenge to generate a diverse set of image captions that are all high-quality.

When researchers choose to use a diverse decoding strategy, they must decide where on the quality-diversity tradeoff they would like to lie; selecting an optimal method depends strongly on one's tolerance for errors.
In machine translation, where mistakes could severely impact coherence, beam search-based methods, which tend to result in better fluency and coherence, but worse diversity might be preferred.  
In more open-ended applications, where novel text is of greater importance, increased diversity could be worth the fluency and coherency hit.
As state-of-the-art models continue to improve, one would hope that the quality cost of encouraging diversity will continue to decrease. 

In the interest of reporting a single overall best method for each task, we computed a sum-of-ranks score for each method.
For dialog, we ranked the methods each by fluency, coherence, interestingness, and Ent-4, and then took a weighted sum of the four ranks, with 50\% of the weight assigned to Ent-4, and 50\% distributed evenly among the human evaluation ranks.
Overall, clustered beam search and standard BS (beam size 100, PDC) have the best scores, followed by clustered beam search (beam size 10).
Similarly, for image captioning, we rank the methods by their mean SPICE score and by Ent-4.
Summing these ranks, random sampling (temp 1.0, top-10 capping, PDC) came in first.
Standard beam search, Hamming Diversity beam search, and Top-$g$ capping beam search (beam size 10) tied for second.

\subsection{Random Sampling-based Methods}
Higher sampling temperatures result in both an increase in diversity in generated responses and a reduction in overall quality.
In the dialog domain, evaluators consistently rate the responses sampled with temperature 1.0 to have worse fluency, coherence, and interestingness when those sampled with temperature 0.5.
In the image captioning domain, lower temperature improves automatic evaluation metrics for quality while reducing diversity.

For dialog, restricting sampling to the top-10 vocabulary words is a more effective strategy than adjusting temperature for ensuring balance between the quality and diversity of outputs.
Top-10 random sampling has the highest fluency, coherence, and interestingness, as well as significantly lower perplexity than other random sampling methods.
However, this trend did not extend to image captioning, where top-10 random sampling results in both worse SPICE scores and lower diversity measures than setting the temperature to 0.7.
This may be because image captioning is a less ambiguous task than open-ended dialog, leading to a better-trained model that puts more probability mass on high-quality vocabulary words, ameliorating the challenge top-$c$ filtering is designed to eliminate: that of a long tail of low probability vocabulary words taking up a large amount of probability mass.

\subsubsection{Beam Search-based Methods}

For dialog, clustered beam search (Cluster5 BS) performs the best of all beam search methods in terms of human-judged interestingness. It ties for best with NPAD0.3BS on fluency and ties with Standard BS on coherence.
Iterative beam search (Iter5 BS) achieves the greatest diversity, but at the expensive of quality.
It has the lowest human-judged coherence among beam search methods; thus, we do not evaluate this method on image captioning.
For image captioning, Cluster5 BS has the highest diversity among beam search methods, but this difference is quite small.
Cluster5 BS also has the highest SPICE@10 score, indicating it is the best method for generating at least one high quality candidate.
However, Top3Cap BS results in the highest mean SPICE score, suggesting it is best at ensuring all outputs are reasonable quality.
% This indicates that for rereanking tasks 

\subsubsection{Effect of Over-sampling}

In our experiments, we explore over-sampling 100 outputs, and then either using post-decoding clustering (PDC) or re-ranking by log-likelihood to filter these 100 down to 10 diverse outputs.

In the dialog domain, this over-sampling approach is a definite win.
When over-sampling with random sampling both methods of filtering substantially improve human judgements of fluency and adequacy compared to random sampling only 10 outputs.
However, interestingness scores go down, and while the outputs are still more diverse than beam search-based methods, they are less diverse than random sampling without filtering.
In the beam search methods that use a beam size of 100 then filter down to 10, human-judged quality is on par with beam size 10 results, but diversity is considerably higher.

When comparing the two types of filtering, PDC results in higher interestingness and diversity statistics, while log-likelihood re-ranking improves fluency and adequacy.
This again demonstrates the trade-off between quality and diversity.\footnote{In the appendix, we show results with every method where we generate 10 samples; generate 100 samples followed by selecting the 10 most likely outputs; and generate 100 samples followed by post-decoding clustering to select 10 outputs.}

For image captioning, over-sampling with reranking does not consistently improve quality as it does in the dialog domain.
Mean SPICE score is improved for random sampling but not for beam search. 
SPICE@1 becomes worse for both random sampling and decoding, while SPICE@10 improves for random sampling, and for beam search when PDC is applied.
From these results, we can conclude that over-sampling then ranking does not have a sizeable effect, either negative or positive, on quality.
Moreover, the diversity of the captions generated by random sampling actually decreases when oversampling. The diversity of beam search-generated captions does improve with over-sampling.

While oversampling does generally improve outcomes on the diversity/quality tradeoff, it is more computationally expensive, particularly with beam search.
Running PDC also requires generating sentence embeddings for every output, which adds additional computation time.



\subsection{Summary of Contributions}
The work described in this section was published in the 2019 Proceedings of the Association of Computational Linguistics \citep{ippolito2019comparison}.
The work was performed with in conjunction with Reno Kriz, with the assistance of Maria Kustikova and the mentorship of Jo{\~a}o Sedoc and Chris Callison-Burch.
Reno and I contributed equally to the development of the ideas in the paper and the design and implementation of the experiments.

\section{Impact of Decoding Strategy on the Detection of Machine-Generated Text}
\label{section:detection}


State-of-the-art generative language models are now capable of producing multi-paragraph excerpts that at a surface level are virtually indistinguishable from human-written content \citep{zellers2019defending,radford2019language,adelani2020generating}.
Often, only subtle logical fallacies or idiosyncrasies of language give away the text as machine-generated, errors that require a close reading and/or domain knowledge for humans to detect.

Deceptive text, whether human- or machine-generated, has entered the sphere of public concern \citep{cooke2018fake}.
It propogates quickly \citep{vosoughi2018spread}, sets political agendas \citep{vargo2018agenda}, influences elections \citep{allcott2017social}, and undermines user trust \cite{wang2012serf, song2015crowdtarget}.
Recently, \citet{adelani2020generating} have shown that automatically generated reviews are perceived to be as fluent as human-written ones.
As generative technology matures, authors, well-meaning or otherwise, will increasingly employ it to augment and accelerate their own writing.
% It is more imperative now than ever for both humans and automated systems to be able to detect and identify machine-generated texts in the wild.
However, there has thus been little inquiry into the textual properties that cause humans to give generated text high human-like ratings compared to those that cause automatic systems to rate it highly.

This task of trying to guess whether text is coming from a robot or a fellow human was made famous by the Turing Test \citep{turing1950computing}.
It continues to be used is chatbot evaluation \citep{lowe2017towards}.
The related (but not identical) task of asking human raters to judge the quality of machine-generated excerpts remains the gold-standard for evaluating open-domain generation systems \citep{van2019best}.

In turns out that even with a fixed language model, choice of decoding strategy has a huge impact on the detectability of generated text.
Using top-$k$ random sampling, a decoding method where only the selection of high-likelihood words is permetted, means we are less likely to make a poor choice and create the type of mistakes that are easy for humans to detect.
Since humans are not proficient at identifying when a model subtly favors some utterances more often than a human author would, they don't notice the over-representation of high-likelihood words in the generated text.
In contrast, automatic systems excel at identifying statistical anomalies and struggle to build deeper semantic understanding.
Top-$k$ in particular creates text that is easy for machines to detect but very hard for humans.
Thus, we observe the general trend: as the number of unlikely words available to be chosen is increased, humans get {\em better} at detecting fakes while automatic systems get {\em worse}.

In this work, we study three popular random decoding strategies---top-$k$, nucleus, and temperature sampling---applied to GPT-2 \cite{radford2019language}.
We draw a large number of excerpts generated by each strategy and train a family of BERT-based \cite{devlin2018bert} binary classifiers to label text excerpts as human-written or machine-generated.
We find large differences in human rater and classifier accuracy depending on the decoding strategy employed and length of the generated sequences.
Regardless of strategy, we find human raters achieve significantly lower accuracy than the automatic discriminators.
We also show that when a decoding strategy severely modifies the unigram token distribution, as top-$k$ does, humans have trouble detecting the resultant generated text, but automatic classifiers find it the easiest to discriminate.
Worryingly, we further find that classifiers are brittle; they generalize poorly when trained to discriminate samples from one strategy and then evaluated on samples from another.

\subsection{Detection as a Task}
The rise of machine-generated content has led to the development of automated systems to identify it.
\textsc{Grover} was designed to not only generate convincing news excerpts but to also identify them using a fine-tuned version of the generative model itself \citep{zellers2019defending}.
GLTR, expecting attackers to use sampling methods that favor high-likelihood tokens, aims to make machine-generated text detectable by computing histograms over per-token log likelihoods \citep{gehrmann2019gltr}.
\citet{bakhtin2019real} frame human-text detection as a ranking task and evaluate their models' cross-domain and cross-model generalization, finding significant loss in quality when training on one domain and evaluating on another.
\citet{schuster2019we} argue that the language distributional features implicitly or explicitly employed by these detectors are insufficient; instead, one should look to explicit fact-verification models.
Finally, discriminators for whether text is machine-generated are a promising research  direction in adversarial training \citep{lin2017adversarial,li2017adversarial} and in automatic evaluation of generative model quality \citep{novikova2017we,kannan2017adversarial,lowe2017towards}. 

We frame the detection problem as a binary classification task: given an excerpt of text, label it as either human-written or machine-generated.
In particular, we are interested in how variables such as excerpt length and decoding strategy impact performance on this classification task. 
We thus create several datasets.
Each is approximately balanced between positive examples of machine-generated text and negative examples of human-written text.
While they all share the same human-written examples, each dataset contains a different set of machine-generated examples sampled using one particular decoding strategy.
We also build additional datasets by truncating all of the examples to a particular sequence length,

By training a separate classifier on each dataset, we are able to answer questions about which decoding strategy results in text that is the easiest to automatically disambiguate from human-written text.
We are also able to answer questions about how the length of the examples in the training set impacts our ability to automatically classify excerpts of that same length as either human-written or machine-generated.

\subsection{Method}
\subsubsection{Dataset Construction}
All of our generated text samples are drawn from GPT-2, a state-of-the-art Transformer-based generative language model that was trained on text from popular web pages \citep{radford2019language}.
While we use the GPT-2 \textsc{Large} model with 774M parameters, we found that similar trends to those reported here hold in experiments with smaller language models.

Given an autoregressive language model that defines a probability distribution over the next token given the previous tokens in a sequence, a decoding strategy generates text by deciding how to output a token at each step based on the predicted distributions.
Perhaps the most straightforward decoding strategy is to randomly choose a token with probability proportional to its likelihood.
A challenge with the random sampling approach is that these probability distributions  often contain a long tail of vocabulary items that are individually low-probability but cumulatively comprise a substantial amount of probability mass.
\citet{holtzman2019curious} observe that choosing tokens from this tail often leads to incoherent generations.

Top-$k$ sampling, nucleus sampling, and (in the extreme) beam search have all been proposed to heuristically promote samples with higher per-token likelihoods.
Top-$k$ and nucleus sampling both do so by setting the likelihood of tokens in the tail of the distribution to zero.
Top-$k$ restricts the distribution to all but the $k$ most likely tokens, where $k$ is a constant \citep{fan2018hierarchical}.
Nucleus sampling, also called top-$p$, truncates the distribution at each decoding step $t$ to the $k_t$-most-likely next tokens such that the cumulative likelihood of these tokens is no greater than a constant $p$ \citep{holtzman2019curious}.

We thus consider three different decoding strategy settings:
\begin{itemize}[noitemsep,topsep=0pt]
  \item Sample from the untruncated distribution
  \item Top-$k$, choosing $k$=40 \citep{radford2019language}.
  \item Nucleus sampling (aka top-$p$), choosing $p$=0.96 \citep{zellers2019defending}.
\end{itemize}

In addition, we form ``negative" examples of human-written text by taking excerpts of web text that come from the same distribution as GPT-2's training data.\footnote{\url{https://github.com/openai/gpt-2-output-dataset}}
By picking text that resembles GPT-2's train set, we ensure that our classifiers can't simply take advantage of stylistic differences between the human-written text corpus and the kind of text GPT-2 was trained to generate.

For each decoding method, we construct a training dataset by pairing 250,000 generated samples with 250,000 excerpts of web text.
5,000 additional paired samples are kept aside for validation and test datasets.
Lastly, we filter out excerpts with fewer than 192 WordPiece tokens \citep{wu2016google} (excerpts might be quite short if the model produces an end-of-text token early on). See Appendix~1 for final dataset sizes.

A crucial question when generating text with a language model is whether or not to provide a priming sequence which the language model should continue.
Unconditioned samples, where no priming text is provided, in conjunction with top-$k$ sampling, lead to pathological behavior for discriminators as the first token of the generated text will always be one of $k$ possible options.
On the other hand, if long sequences of human text are used as priming, the space of possible generated sequences is larger, but the detection problem shifts from one of ``how human-like is the generated text?" to ``how well does the generated text follow the priming sequence?".

Since in this study we are interested in the former simpler question, we create two datasets, one with no priming, and one with the minimum amount of priming possible: a single token of web text.
This means that for every excerpt of web text in the training set, there is an excerpt of machine-generated text that starts with the same token.
We find that even with limited priming, the ability of automatic detectors can be strongly impacted.

To study the effect of excerpt length, we construct variations of the above datasets by truncating all excerpts to ten possible lengths ranging from 2 to 192 WordPiece tokens \cite{wu2016google}. In total, we obtain sixty dataset variations: one per sampling method, truncation length, and choice of priming or no priming.

% Table generated by Excel2LaTeX from sheet 'simple_baselines'

\begin{table}[t]
  \centering
  \small
    \begin{tabular}{|l||cc||cc|cc|cc|c||c|}
    \hline
          & \multicolumn{2}{c||}{BERT} & \multicolumn{2}{c|}{BagOfWords} & \multicolumn{2}{c|}{HistGLTR} & \multicolumn{2}{c|}{Hist50Buckets} & \multicolumn{1}{l||}{TotalProb} & \multicolumn{1}{l|}{Human} \\
    Method & \multicolumn{1}{l}{acc} & \multicolumn{1}{c||}{AUC} & \multicolumn{1}{c}{acc} & \multicolumn{1}{c|}{AUC} & \multicolumn{1}{c}{acc} & \multicolumn{1}{c|}{AUC} & \multicolumn{1}{c}{acc} & \multicolumn{1}{c|}{AUC} & \multicolumn{1}{c||}{acc} & \multicolumn{1}{c|}{acc}\\
    \hline
    k40-1wordcond & 0.88  & 0.99  & 0.79  & 0.87  & 0.52  & 0.52  & 0.69  & 0.76  & 0.61 & 0.64 \\
    p0.96-1wordcond & 0.81  & 0.89  & 0.60  & 0.65  & 0.53  & 0.56  & 0.54  & 0.56  & 0.63 & 0.77 \\
    p1.0-1wordcond & 0.79  & 0.92  & 0.59  & 0.62  & 0.53  & 0.55  & 0.54  & 0.55  & 0.65 & 0.71\\
    % What amount of precision do we want?
    % k40-1wordcond & 0.876 & 0.987 & 0.791 & 0.866 & 0.515 & 0.522 & 0.695 & 0.757 & 0.608 \\
    % p0.96-1wordcond & 0.812 & 0.895 & 0.604 & 0.650 & 0.533 & 0.562 & 0.543 & 0.559 & 0.626 \\
    % p1.0-1wordcond & 0.793 & 0.924 & 0.591 & 0.621 & 0.529 & 0.548 & 0.535 & 0.545 & 0.655  \\
    \hline
    \end{tabular}%
  \caption{Performance (accuracy and AUC) of the fine-tuned BERT classifier and several simple baselines on detecting length-192 sequences generated with one word of priming (1worccond). Note that p1.0 refers to untruncated random sampling, where we sample from 100\% of the probability mass. The last column shows human performance on the same task where accuracy with a 50\% baseline is computed by randomly pairing samples from each decoding strategy with a human-written sample.}
  \label{tab:baselines}%
\end{table}%

\subsubsection{Methods for Automatic Detection}
The primary discriminator we employ is a fine-tuned BERT classifier \citep{devlin2018bert}.
We fine-tune one instance of BERT per dataset variation described above.
For the longest sequence length, $n$=192, we compare BERT's performance with several simple baselines that have been proposed in other work.

\paragraph{Fine-tuned BERT}
We fine-tune BERT-\textsc{Large} (cased) on the task of labeling a sentence as human- or machine- generated.
The models are trained for 15 epochs, with checkpoints saved every 1000 steps, and a batch size of 256.
All results are reported on the test set using the checkpoint for which validation accuracy was highest.

\paragraph{Bag-of-Words}
% \textsc{BoWDisc} 
For each sequence, we compute a bag-of-words embedding where each dimension corresponds to a token in GPT-2's  50,000 token BPE vocabulary \citep{sennrich2016neural}, and we count how many times that token appears in the text sequence. 
We then train a logistic regression binary classifier to predict human- or machine-written given this 50,000-dimensional embedding.
We experimented with truncating embedding size by removing entries for infrequent vocabulary words, but this did not improve performance.

\paragraph{Histogram-of-Likelihood Ranks}
Following GLTR \citep{gehrmann2019gltr}, we compute the probability distribution of the next word given the previous words in a text sequence according to a trained language model (in our case the same GPT-2 model that was used for generation).
At each sequence position, we rerank the vocabulary words by likelihood, and record the rank of the ground-truth next word within this list.
These ranks are then binned.
GLTR uses four bins, counting (1) the number of times the top 1 word is seen, (2) the number of times words ranked 2 through 5 are seen, (3) words ranked 6-100, and (4) words ranked \textgreater100.
However, we observe higher accuracy when 50 bins are spread uniformly over the possible rankings.
This means that since there are 50,000 vocabulary words, the first bin counts the number of times the actual next word was within the 1,000 mostly likely next words, the second bin counts the 1,001-2,000th, and so on.
We then train logistic regression binary classifiers to predict human- or machine-written given either the 4-dimensional histograms or 50-dimensional histograms as input.

\paragraph{Total Probability}
\citet{solaiman2019release} propose a very simple baseline consisting of a threshold on the total probability of the text sequence.
An excerpt is predicted as machine-generated if its likelihood according to GPT-2 is closer to the mean likelihood over all machine-generated sequences than to the mean of human-written ones.

\subsubsection{Method for Human Detection}
The human evaluation task is framed similarly to the automatic one.
We ask the raters to decide whether a passage of text was written by a human or by a computer algorithm. (Full instructions are in the Appendix.)
Raters are allowed to choose between four options: ``definitely" or ``possibly" machine-generated and  ``definitely" or ``possibly" human-written.
They are first shown an excerpt of length 16 WordPiece tokens.
After they make a guess, the length of the excerpt is doubled, and they are asked the same question again.
This continues until the entire passage of length 192 tokens is shown.
Passages are equally likely to be human-written or machine-generated, with the machine-generated excerpts being evenly split between the three sampling strategies considered in this paper.

Initially, Amazon Mechanical Turk (AMT) raters were employed for this task, but rater accuracy was poor with over 70\% of the ``definitely" votes cast for ``human" despite the classes being balanced.
Accuracy, even for the longest sequences, hovered around 50\%.
The same study was then performed with university students who were first walked through ten examples (see Appendix Table 4) as a group.
Afterward, they were asked to complete the same tasks that had been sent to the AMT workers.
No additional guidance or direction was given to them after the initial walk-through.
We will refer to this group as the ``expert raters."
Among them, 52.1\% of ``definitely" votes were cast for human, and accuracy on the longest excerpt length was over 70\%.

\begin{figure}[t]
\begin{subfigure}{.495\textwidth}
    \center
    \includegraphics[width=\textwidth]{figures/bert_accuracy}
    \caption{}
    \label{fig:bert_accuracy} 
\end{subfigure}
\begin{subfigure}{.495\textwidth}
    \center
    \includegraphics[width=\textwidth]{figures/fraction_incorrect_fp}
    \caption{}
    \label{fig:errors} 
\end{subfigure}
\caption{In \textbf{(a)}, accuracy increases as the length of the sequences used to train the discriminator is increased.
In \textbf{(b)}, we see that the BERT fine-tuned discriminator predicts about the same number of false-positives as false-negatives when trained with samples generated using top-$p$ sampling. However, for top-$k$, it more often mistakes machine-generated text to be human-written, while for untruncated random sampling the opposite is the case.}
\end{figure}

The human evaluation dataset consisted of 150 excerpts of web text and 50 excerpts each from the three decoding strategies.
Each question was shown to at most three raters, leading to 900 total annotations from the untrained workers and 475 from the expert raters.
A more detailed breakdown can be found in the Appendix.


\subsection{Results}
\label{section:auto_detection}
\paragraph{Simple Baselines}
Table \ref{tab:baselines} shows the performance of the baseline discriminators on length-192 sequences, as compared with fine-tuned BERT.
Reassuringly, BERT far surpasses all simple baselines, indicating that it is not fully possible to solve the detection problem without complex sequence-based understanding.
The simplest baseline, TotalProb, which makes a decision based on the likelihood of the sequence, performs surprisingly well (over 60\% accuracy for all sampling methods) relative to the methods which involve training logistic regression models.

Logistic regression on bag-of-words is the best of the baselines, beating out the histogram-based methods.
While \citet{gehrmann2019gltr} report an AUC of 0.87 on classifying text as real or generated using logistic regression on the four buckets of the GLTR system, we report AUC between 0.52 and 0.56 for this task.
The discrepancy is likely due to the fact that the human-written text in our discriminator training set comes from the same distribution as the text used to train the language model, while in GLTR the human text comes from children's books, scientific abstracts, and newspaper articles. 
The selection of training data for learned detection systems is crucial. In real-world applications, the choice ought to reflect the genres that builders of text-generation systems are trying to impersonate. 

\begin{figure}[t]
    \begin{subfigure}{.45\textwidth}
        \center
        \includegraphics[width=\textwidth]{figures/mean_ks}
        \caption{}
        \label{fig:mean_ks} 
    \end{subfigure}
    \begin{subfigure}{.45\textwidth}
        \center
        \includegraphics[width=\textwidth]{figures/token_count_histogram}
        \caption{}
        \label{fig:token_count_histogram} 
    \end{subfigure}
    \caption{In \textbf{(a)}, the average (over sequences in the test set) $k$ chosen at each step during generating with nucleus sampling is plotted. Adding a single word of priming strongly impacts the $k$s chosen for the first few positions, but this difference quickly dissipates.
    In \textbf{(b)}, we consider the first token generated in each sequence by top-$k$, and plot what fraction of these are captured by the $k$ most common unique tokens from the vocabulary. Overall, at its first step, top-$k$ concentrates 80\% of its probability mass in the 500 most common tokens from the vocabulary.}
\end{figure}

\paragraph{Fine-tuned BERT} In Figure~\ref{fig:bert_accuracy}, we begin by observing discriminator accuracy as a function of excerpt length and sampling method.
As can be intuitively expected, as sequence length increases, so too does accuracy.
For unconditioned text decoded with nucleus (p0.96) and untruncated (p1.0) random sampling, we find discriminator accuracy increases from 55\%, near random, to about 81\% for the longest sequences tested.
In contrast, discriminators trained and evaluated on top-$k$ achieve over 80\% accuracy even on 16-token excerpts.

Why are top-$k$'s samples so easy to detect?
In Figure~\ref{fig:token_count_histogram}, we see the percentage of probability mass concentrated in the $k$ most common token types for each sampling method.
While random sampling and nucleus sampling are very similar to human-written texts, we see top-k concentrating up to 80\% of its mass in the first 500 most common tokens.
The other sampling methods as well as human-written texts require at least 1,100 token types for the same.
It is clear that top-$k$'s distribution over unigrams strongly diverges from human-written texts--an easy feature for discriminators to exploit.
In fact, \citet{see2019massively} note that it takes setting $k$ to 1000 to achieve about the same amount of rare word usage and fraction of non-stopword text as as human writing.\footnote{when decoding from the GPT-2 small model with 117M parameters.}
This makes it very easy for the model to pick out machine-generated text based on these distributional differences.

One way to help resolve this problem is to add priming text.
Doing so causes more rare words to be incorporated into the top-$k$ of the unigram distribution.
Adding even a single human word of priming significantly reduces the performance of detectors trained with top-$k$ random sampling.
Without priming, a discriminator trained on sequences of length 2 can classify with $\mathtt{\sim}$90\% accuracy the provenance of the text (Figure \ref{fig:bert_accuracy}).
By adding one priming token, accuracy drops to $\mathtt{\sim}$65\%.
Even on the longest 192-length sequences, top-$k$ discriminator accuracy is 6\% lower on the primed dataset than the unprimed one.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/human_merged}
    \caption{\textbf{(a)} and \textbf{(b)} show human rater accuracy of correctly identifying an excerpt as human-written or machine-written, shown with 80\% confidence internals, in \textbf{(a)}, broken up by decoding strategy and in \textbf{(b)}, overall. Accuracy increases as raters observe more tokens. \textbf{(c)} shows that for short excerpts, most rater mistakes are them incorrectly thinking machine-generated text is human written. The two errors types become more balanced at longer lengths.}
    \label{fig:human_eval} 
\end{figure}

When generating with nucleus or untruncated random sampling, adding a priming token is not as impactful, as these methods are already sampling from a large fraction (or all) of the probability distribution.
This is seen in Figure \ref{fig:mean_ks} where at the very first step of unprimed generation, nucleus sampling selects from 3075 possible vocabulary words, and at later positions selects from on average more than 500.
Untruncated random sampling always selects from the entire 50,000 word vocabulary, whereas top-$k$ only selects from $k$.


\paragraph{Transferability}
In Table \ref{tab:transfer_accuracy}, we show how discriminators trained with samples from one decoding strategy can transfer at test time to detecting samples generated using a different decoding strategy.
Unsurprisingly a discriminator trained on top-$k$ generalizes poorly to other sampling methods: accuracy drops to as low as 42.5\%, \textit{worse than chance}.
Conversely, training the discriminator with sequences sampled from the untruncated distribution leads to little transferability to detecting top-$k$ samples.
Only the discriminator trained with nucleus sampling (a compromise between unmodified sampling and top-$k$) was able to detect sequences from the other sampling strategies without too much of a hit to accuracy.
As expected, a discriminator trained on an equal portion of data from each decoding method does reasonably at detecting all three.


\begin{table}[t]
  \small
  \centering
  \begin{tabular}{|c c|c|c|c|}
    \hline
    & & \multicolumn{3}{c|}{Eval} \\
    \cline{3-5}
    &            & top-$k$         & nucleus & random \\
    \hline
    \multirow{3}{0.7em}{\rotatebox[origin=c]{90}{\parbox[c]{1cm}{\centering Train}}} 
    & \multicolumn{1}{|c|}{top-k}   & \textbf{90.1} & 57.1          & 43.8 \\
    & \multicolumn{1}{|c|}{nucleus} & 79.1          & \textbf{81.3} & 78.4 \\
    & \multicolumn{1}{|c|}{random}  & 47.8          & 63.7          & \textbf{81.7} \\
    \hline
    & \multicolumn{1}{|c|}{mixed}  & 88.7          & 74.2          & 72.2 \\
    \hline
  \end{tabular}
  \caption{Accuracy of BERT fine-tuned discriminator when trained on samples from one strategy (rows) and evaluated on another (columns). Trained on samples with 192 tokens. The `mixed' dataset is one containing an equal portion of samples from each strategy.}
  \label{tab:transfer_accuracy}
\end{table}

\begin{table}[t]
  \small
  \centering
  %                     p(machine)                              
  % dataset         k0-1wordcond k40-1wordcond p0.96-1wordcond
  % model                                                     
  % k0-1wordcond        0.382658      0.073189        0.226300
  % k40-1wordcond       0.145253      0.608884        0.278946
  % p0.96-1wordcond     0.488926      0.491959        0.517189
  \begin{tabular}{|c c|c|c|c|}
    \hline
    & & \multicolumn{3}{c|}{Eval} \\
    \cline{3-5}
    &            & top-$k$         & nucleus & random \\
    \hline
    \multirow{3}{0.7em}{\rotatebox[origin=c]{90}{\parbox[c]{1cm}{\centering Train}}} 
    & \multicolumn{1}{|c|}{top-k}   & 60.9 & 27.9 & 14.5 \\
    & \multicolumn{1}{|c|}{nucleus} & 49.2 & 51.7 & 48.9 \\
    & \multicolumn{1}{|c|}{random}  &  7.3 & 22.6 & 38.3 \\
    \hline
  \end{tabular}
  \caption{Average probability of `machine-generated' according to each length-192 discriminator. The expected in-domain probability is 0.5. One token of conditioning.}
  \label{tab:transfer-prediction}
\end{table}

Perhaps this lack of transferability is related to each discriminator's calibration.
Indeed, the degree to which a discriminator's average prediction deviates from 50\% is a direct indicator of its accuracy. 
In Table~\ref{tab:transfer-prediction}, we observe that of the three BERT discriminators, only that trained on top-$p$ samples predicts `machine-generated' on approximately 50\% of in-domain examples as expected. 
This same discriminator's behavior holds on datasets generated by other sampling strategies as well. 
In contrast, we observe that discriminators trained on top-k and untruncated random samples severely underestimate the percentage of machine-generated excerpts in out-of-domain datasets.
Even within domain (Figure~\ref{fig:errors}), we find both discriminators heavily favor a single class, increasingly so as the number of tokens increases.

% False-negative = machine-written labeled as human-written
% Negative = machine-written
% Positive = human-written

\begin{table}
\setlength\tabcolsep{4.5pt}
    \small
    \centering
    \begin{tabular}{|p{0.40in}|p{0.42in}|p{0.35in}|p{0.35in}|p{0.35in}r|p{0.40in}|p{0.42in}|p{0.35in}|p{0.35in}|p{0.35in}r|}
\hline
\textbf{Truth} & \textbf{Raters} & \textbf{p1.0} & \textbf{k40} & \textbf{p0.96} & &
\textbf{Truth} & \textbf{Raters} & \textbf{p1.0} & \textbf{k40} & \textbf{p0.96} & \\
\hline
H & M & H & H & M & &
H & H & M & M & M & \\
\multicolumn{6}{|p{2.88in}|}{
\tiny
EDIT:OKAY!, I guess that'll work for now. \textgreater\_ http://www.teamfortress.com/ and then go buy the game and experience some of the best online gaming I have ever played. \textasciicircum\_\_\textasciicircum Both girls had a really fun time and I had a GREAT time making both of these costumes. Everything was altered even a little bit(dying the pants a darker grey and painting the boots and shirts) But my piece de resistance would have to be my eldest's Medi-Gun.If you have any questions about the costumes, I would be happy to assist you!Oh and here's a video of my daughter before the costume was completed.Thanks!
}&
\multicolumn{6}{p{3.03in}|}{
\tiny
Image copyright Getty Images Image caption Women mourn over the coffin of one of the victim's of Sunday's bombing in Ankara \textparagraph Who'd be in Turkey's shoes right now? \textparagraph Since July last year, hundreds of soldiers and civilians have been killed in terrorist attacks. Suicide bombs have torn into crowds of demonstrators and tourists. Military convoys have been targeted in the heart of the capital. \textparagraph A long-running Kurdish insurgency, once thought to be close to resolution after years of painstaking efforts to build bridges, has erupted once more. \textparagraph The country is awash with Syrian and other refugees. The government has been under pressure to stop them moving on into Europe and prevent would-be jihadis travelling the other way. \textparagraph How dangerous is Turkey's unrest? \textparagraph Tears and destruction amid PKK crackdown \textparagraph Turkey v Islamic State v the Kurds
}\\
\hline
\hline
\textbf{Truth} & \textbf{Raters} & \textbf{p1.0} & \textbf{k40} & \textbf{p0.96} & &
\textbf{Truth} & \textbf{Raters} & \textbf{p1.0} & \textbf{k40} & \textbf{p0.96} & \\
\hline
        M & M & H & - & - & &
        M & M & - & - & H & \\
\multicolumn{6}{|p{2.88in}|}{
\tiny
First off, this thread has done a pretty good job of describing in detail yet another broken touchscreen. That's the difference between a smartphone and a PC with no prying eyes having to snap shots for the police to find. \textparagraph What I would like to address is the mindset that generally surrounds Chrome OS users. To me this is analogous to saying that Apple does``hate their Windows", or that HP does``hate their Macs" as if http://twitter.com/) (and that quote is from two years ago), that anyone who covers smartphones and tablets from a ``PC" perspective is just jealous. \textparagraph Chrome OS is for browsing the web, PC processors can do stronger things in that regard, Windows is a juggernaut on those fronts. This is how I see it. Yes, it can be slow. And yes, you need a fast CPU
}
&
\multicolumn{6}{p{3.03in}|}{
\tiny
FOR ALABAMA, GOOD WEEKS \textparagraph AND A TOUR OF CAIRO \textparagraph THE ALABAMA COMMITTEE ON THE STUDY OF THE AMERICAN SECURITY AGENDA, \textparagraph America's future has been mapped out in carved stone. Metro Atlanta's last US congressman, Bill Posey, was a inextricable integral element of the Citadel project as it became another metaphor for Atlanta's transformation from an industry backwater into the finance and information hub of the nation's capital. Meanwhile, Cobb County -- Atlanta's geode of change -- is home to some of the largest industrial parks in the South, a regional cultural center, a 100-year-old manufacturing town and a potent symbol of the former city's cherished Georgian past. The gentry still live there, the defunct industrial landscapes carry the names of
}\\
\hline
\hline
\textbf{Truth} & \textbf{Raters} & \textbf{p1.0} & \textbf{k40} & \textbf{p0.96} & &
\textbf{Truth} & \textbf{Raters} & \textbf{p1.0} & \textbf{k40} & \textbf{p0.96} & \\
\hline
        M & H & - & - & M & &
        M & H & - & M & - & \\
\multicolumn{6}{|p{2.88in}|}{
\tiny
Exidentia at Eurnari, is an upcoming Cryptopia event which is currently still in development. Be a part of the first live stream of this year's event on 15-16 January 2016! \textparagraph Since the release of v1.22, Exidentia has received a fair amount of user feedback. This event takes place in the underwater Cryptopia they have built. During this event, you will learn about the ocean and areas around it, and be reached by a treasure hunter that helps you explore the different areas. \textparagraph There will be six different levels in this event that you will become acquainted with: thought Polar Lava, Ocean Seared Cones and Celestine Floors, Sea Damaged Aerie Bricks, coast Puddle (congipit stopping at red water), Shaikh Swamp and Bugmite. At rotating points, you will learn how to access various types of creatures
}
&
\multicolumn{6}{p{3.03in}|}{
\tiny
Ever since the opening of the North American College of Art Education in 1990, the demand for art education in America has grown steadily, and in recent years we have seen the rise of students that pursue art education not in the classroom but at art academies. This year saw another 50 percent increase in the number of art academies in the United States offering courses -- with an additional 10 percent of students in 2017 taking art. \textparagraph Some major changes have occurred in recent years with regard to the art curriculum and the way students learn, and we will explore each of these in coming months as we look at the various forms of art education. There is no one-size-fits-all approach for this or any other field of study, and students who begin a course in art education may change their plans based on what they see that course, including what lessons they have completed and the resources available, to create meaningful experiences of artistic creation. \textparagraph One important area
}\\
\hline

    \end{tabular}
    \caption{Some 192-token examples where at least two expert raters agreed with each other, but were not in agreement with the automatic discriminators. The first row shows examples where the ground-truth was human-written, the second shows machine-generated examples where the corresponding discriminator guessed incorrectly, and the third shows machine-generated examples where the discriminator was correct, but raters got it wrong.}
    \label{tab:qual_examples}
\end{table}

\paragraph{Human Evaluation}
Overall human performance across all sampling methods is shown in Figure \ref{fig:human_eval}b.
Even with the multi-paragraph 192-length excerpts, human performance is only at 71.4\%, indicating that even trained humans struggle to correctly identify machine-generated text over a quarter a time.
However, it is worth noting that our best raters achieved accuracy of 85\% or higher, suggesting that it is possible for humans to do very well at this task.
Further investigation is needed into how educational background, comfort with English, participation in more extensive training, and other factors can impact rater performance.

To break up the accuracies by sampling method in a way that is comparable to the results shown for the automatic discriminators, we pair each machine-generated example with a randomly selected one of webtext to create a balanced dataset for each sampling strategy.
Performance is shown in Figure \ref{fig:human_eval}a.
Top-$k$ produces the text that is hardest for raters to correctly distinguish, but as shown in Section \ref{section:auto_detection}, it is the easiest for our automatic detection systems.
Samples from untruncated random sampling and nucleus sampling with $p$=0.96 are equivalently difficult for raters to classify as machine-generated.
Our human evaluation results suggest that much lower $p$-values than the 0.92 to 0.98 range proposed in \citet{zellers2019defending} might be necessary in order to generate text that is considered significantly more human-like to human raters than the text produced by using the untruncated distribution.

Table \ref{tab:qual_examples} gives several examples where human raters and our BERT-based discriminators disagreed.
When raters incorrectly labeled human-written text as machine-generated, often the excerpts contained formatting failures introduced when the HTML was stripped out.
In the middle two examples, topic drift and falsehoods such as Atlanta being the ``information hub of the nation's capital" allowed humans to correctly detect the generated content.
However, in the bottom two examples, the high level of fluency left human raters fooled.

Overall we find that human raters---even ``expert" trained ones---have consistently worse accuracy than automatic discriminators for all decoding methods and excerpt lengths.
In our experiments, randomly-selected pairs of raters agree with each other on a mere 59\% of excerpts on average. (In comparison, raters and discriminators agree on 61\% to 70\% of excerpts depending on the discriminator considered).
We surmise that the gap between human and machine performance will only grow as researchers inevitably train bigger, better  detection models on larger amounts of training data.
While improved detection models are inevitible, it is unclear how to go about improving human performance.
GLTR proposes providing visual aids to humans to improve their performance at detecting generated-text, but it is unlikely that their histogram-based color-coding will continue to be effective as generative methods get better at producing high-quality text that lacks statistical anomalies.

% In this work, we study the behavior of automated discriminators and their ability to identify machine-generated and human-written texts. 
% We train these discriminators on balanced binary classification datasets where all machine-generated excerpts are drawn from the same generative model but with different decoding strategies.
% We find that, in general, discriminators transfer poorly between decoding strategies, but that training on a mix of data from methods can help.
% We also show the rate at which discriminator accuracy increases as excerpts are lengthened.
% 
% We further study the ability of expert human raters to perform the same task.
% We find that rater accuracy varies wildly, but has a median of 74\%, which is less than the accuracy of our best-performing discriminator.
%Most interestingly, we find that human raters and discriminators make decisions based on different qualities, with humans more easily noticing semantic errors and discriminators picking up on statistical artifacts.
% In our experiments, these artifacts are most prominent with top-$k$ sampling.
%However, any strategy that over-samples high-likelihood words is susceptible.
%As the $p$ in nucleus sampling is set increasingly lower to achieve more fluent text (some systems are already using $p$ as low as 0.5 \citep{miculicich2019selecting}), the distributional deviations that plague top-$k$ text will surface in nucleus sampling as well.

%\citet{holtzman2019curious} explain how a unique attribute of human language is that it dips in and out of low probability zones.
%This variance in likelihood is what makes human-written text interesting and exciting to read.
%Today's generation systems have not yet solved the problem of mimicking the human cadence without introducing poor word choices that are easy for humans to detect. 
%Generation systems often optimize for fooling humans without acknowledging the trade-off that exists between human perception of quality and ease of automatic detection.
%We therefore suggest three prongs for future research:

\subsection{Summary of Contributions}
The work described in this section was published in the 2020 Proceedings of the Association of Computational Linguistics \citep{ippolito2020automatic}.
The work was performed with in conjunction with Daniel Duckworth, with the mentorship of Douglas Eck and Chris Callison-Burch.
I proposed the idea of studying detection, and Daniel and I worked both worked on designing and implementing the experiments and analyzing the results.


% \section{Detecting the Boundary between Human-Written and Machine-Generated Text}
\section{\ROFT: A Largescale Study of Human Detection Ability}
\label{section:roft}

\subsection{Introduction}
In the previous section, we conducted a small-scale study showing how choice of decoding strategy impacts human ability to detect machine-generated text.
However, there are many other factors which influence detectability that we were not able to include in this study, including the domain of the text being used for evaluation and the architecture and manner in which the underlying language model was trained.
In addition, we were interested in studying the annoators themselves--how do annotator background as well as the incentive structure set up for soliciting annotations impact performance on the detection task?
We therefore saw the necessity of designing a platform for conducting large-scale studies of the detection task.

Existing studies, including the one in Section \ref{section:detection}, focused on the binary question of whether or not a provided document contains any generated text.
For our large-scale study, we instead framed detection as a boundary-detection task: given a document that starts off as human-written and at some point transitions to machine-generated, can annotators detect the transition point?
The boundary detection setting is more informative than the classification setting because it better aligns with how LMs are used to generate text in practice; in typical usage, a generative system is provided with a prompt and asked to produce a continuation.
By measuring human skill at the boundary detection task, we were able to evaluate the relative performance of different generative systems, build a better understanding of how incentive structure influences the quality of the annotations acquired, and make progress toward quantifying the risks associated with large language model goals..
Furthermore, because the annotation platform we built was public, we could achieve these research goals while simultaneously educating the public about how to spot generated text. 

The primary contributions of this section are as follows:
\begin{itemize}[itemsep=0.5pt,topsep=1pt]
    \item The ``Real or Fake Text'' game, which anyone can play on the internet.
    \item Large-scale case study on how a game-like platform can be used to perform data collection for the detectability task.
    \item Analysis of how generation-time design choices impact detectability of generated text. 
    \item Study of the errors and text properties which humans associate with machine generations.
    \item Public dataset of 21,646 human annotations on the boundary detection task.
\end{itemize}

\begin{figure}[tb]
    \centering
    \framebox{\includegraphics[scale=0.19]{figures/interface.png}}
    \caption{In the boundary detection task, players see one sentence as a time and try to guess when they transition from human-written to machine-generated.}
    \label{fig:pg1}
\end{figure}

\subsection{Related Work}

% Evaluation of natural language generation remains a difficult task to organize. However, the growing subfield that studies the identification of machine-written text has offered a more practical human-centric approach to model evaluation.
Previous research on understanding the ability of humans to detect machine generated text has mostly posed the task as a classification task--given a text example that is either entirely human-written or entirely machine-generated (aside from an initial prompt), annotators must predict whether it is human-written or machine-generated.
On this task, \citet{ippolito-etal-2020-automatic} reported that trained evaluators were able to achieve an accuracy of at best 71.4\%, using generation from GPT-2 Large \citep{radford2019language}.
\citet{clark2021all} demonstrated that annotators are able to distinguish GPT-2 XL generations with at best 62\% accuracy, but they perform no better than random chance on GPT-3 outputs \citep{brownetal2020}.
Even after training evaluators to improve their detection abilities, detection accuracy on GPT-3 was only able to converge to around 55\%.
A study by \citet{brownetal2020} reported similarly low performance (52\%) on the detection of machine-generated news articles.

Our work builds directly off of the RoFT detection game introduced by \citet{roft}; we contribute expanded analysis across a variety of genres and generative systems.
A similar detection task, where annotators guess whether turns in a conversation were generated, was posed as a way to evaluate dialog systems \citep{deriu-etal-2020-spot}.

Another related area of research is asking annotators to explain why they think generated text is generated.
\citet{he-etal-2021-tgea} created a dataset of generated text annotated with the errors that humans found in it, and \citet{dou2021scarecrow} proposed an error annotation schema for generated text.
The errors we allow players to report in our experiments were inspired by this schema.

% In this paper, we expand on the findings of previous research by altering the evaluation methodology from a classification task over an entire text to a boundary detection task, where human evaluators are asked to identify the sentence at which a passage transitions from being human-written to machine generated. We also display the effect of annotator ``training'', where we investigate whether repeated attempts at the task with feedback yields better performance over time. We provide additional insight into detection performance, by controlling for variables such as hyperparameter value, model parameter size, and textual domains.

\subsection{The Real or Fake Text Game}
In order to facilitate data collection, we follow \citet{roft} and pose the detection task as a game.
In each game round, players were shown one sentence at a time and earned points for guessing close to the true boundary (Figure \ref{fig:pg1}).
They were also asked to select reasons for why they made their decision.
In total we collected over 20,000 annotations.
We find that players vary substantially in their detection ability, and that factors such as the amount of time taken to complete a game round and total number of game rounds played sometimes correlate with success.
We discuss the difficulty in incentivizing players to improve over time.
Furthermore, we examine some of the the trends and errors which distinguish real from generated text and look at whether annotators could pick up on these trends.



Our study uses data collected through the ``Real or Fake Text'' (\ROFT{}) annotation platform \citep{roft}.
\ROFT{} is a turn-based game where a player first selects a domain of text (news articles, recipes, short stories, or speeches).
The player then plays a series of game rounds.
In each round, the player is shown a starting sentence which they are told comes from a real human-written document.
They are then shown subsequent sentences, one at a time.
Each subsequent sentence may be the true continuation of the document, or it may be text generated by a language model.
Once the sentences transition to being machine-generated, they will stay so for the rest of the 10-sentence passage.

After being shown each sentence, the player must guess whether that sentence was machine-generated or human-written.
If the user selects ``human-written,'' another sentence is displayed.
If the player deems the current sentence to be written by a machine, the game round ends and the true author (machine or human) for each sentence is revealed.
Before submitting their selection, the player is able to select a reason to explain their choice of sentence\footnote{See Table \ref{tab:reasons_text} in the Appendix for a full list of reasons given with their accompanying description text.}.
Thus, the player's goal in \ROFT{} is to correctly identify the sentence at which a passage transitions from being human written to being generated by a language model.
This setting is considerably more realistic than prior work, since in the real world, generating with a prompt is the standard way to achieve controllability, and malicious actors will not reveal what portion of a generation is the human-written prompt.

\subsection{Experimental Design}

\subsubsection{Datasets}
In order to answer questions of how textual genre and writing style affect detectability of machine-generated text, we selected four diverse categories of prompts. 
For each category, documents were sentence-segmented, and only documents with 11 or more sentences were retained.
For each document, the first $h$ sentences were used as the prompt, where $h$ is a uniform random number between 1 and 10 (inclusive). The remaining $10-h$ sentences of each 10-sentence game round were a machine-generated continuation. Our four genres of prompts are as follows:

\paragraph{News articles.}
Documents were drawn from the New York Times Annotated Corpus \citep{sandhaus2008new}, which contains 1.8 million articles published by the Times between 1987 and 2007.
Our hypothesis was that this domain would be challenging for models since news requires factual accuracy, which state of the art models have been shown to struggle with \cite{nakano2021webgpt, lin2021truthfulqa}. 

\paragraph{Presidential speeches.}
Documents were drawn from the presidential speech corpus \citep{brown2016cops}, which contains 963 speeches given by presidents of the United States, with dates ranging from 1789 to 2015.
Our hypothesis was that the sort of first-person rhetoric found in these speeches would be easy for models to impersonate since political speech and first-person speech are plentiful in web-based training data.

\paragraph{Stories.}
Fictional stories were selected from the Reddit Writing Prompts dataset \citep{fan2018hierarchical}, a corpus of amateur short stories scraped from the r/WritingPrompts sub-Reddit\footnote{\tiny https://www.reddit.com/r/WritingPrompts/}. 
We hypothesized that this domain would be challenging for players since the writing quality of the stories is not especially high (which lowers the bar for the model generation quality), and factuality is not as important in a fictional domain.

\paragraph{Recipes.}
Recipes were extracted from the Recipe1M+ dataset \cite{marin2019learning}.
Recipes were parsed slightly differently than the other domains.
We set the ``first sentence'' of each document as the name of the recipe and the ingredient list, and each subsequent ``sentence'' was a step in the recipe. Some recipe steps were more than one sentence. We hypothesized that this dataset would be difficult for models due to the closed-ended nature of the task and the reliance on common sense. 

\subsubsection{Awarding Points}
\label{sec:points}
In each game round, the player is awarded points based on how close their selection was to the true boundary\footnote{For our purposes, the ``boundary'' sentence is considered to be the first machine-generated sentence in the passage}.
Players were awarded 5 points for correctly choosing the boundary sentence and $\max(5-n, 0)$ points for a guess $n$ sentences after the boundary.
Players were not awarded points for guessing a sentence before the boundary.
Players were able to see how many points they earned in each category on their profile page and compare their performance with fellow players on the leaderboard page.
In the Findings section (Section \ref{section:roft_results}), we report mean score earned as the predominant evaluation metric. 
This metric has high correlation with other more standard metrics (Appendix \ref{app:metric}).
% Users are additionally given profiles, and are able to collect "trophies" for various accomplishments during the annotation task (e.g. getting the boundary exactly correct) in a manner to incentivize additional annotation collection.

\subsubsection{Player Recruitment and Annotation Filtering}
\label{sec:players}
Players were recruited from two sections of an Artificial Intelligence course for Master's students and senior undergraduates at an American university.
We only analyze fully anonymized data from students who consented to having their annotations used for research purposes.

% TODO: Replace with University of Pennsylvania after anonymity period.
The first section (Group A) was asked to play 30 minutes of the \ROFT{} annotation game for a fixed amount of points of class credit.
Students in this section were not given any instructions beyond how to create an account.
The second section (Group B) was explicitly told they would be awarded $\min(p/250, 2)$ points of extra credit toward their final grade, where $p$ was the number of points the student earned on the \ROFT{} leaderboard.
Students in Group B were given detailed instructions\footnote{
% [Link redacted for review process.]
{\tiny{https://storage.cloud.google.com/roft\_datasets/boundary\_detection\_guide.pdf}}
} and examples of signs to look out for that text was machine-generated.
% TODO: Change google doc to be PDF and host on roft cloud bucket
Table \ref{tab:particpants} gives statistics on the annotations collected from each class.

We note that university students taking an advanced artificial intelligence course are not reflective of the global population of English speakers, and the results presented in this paper may not reflect the general population's ability to detect machine-generated text. 

In total, we collected 42,165 annotations over 7,895 different game rounds.
The annotations were then filtered in the following ways.
If a player guessed the same boundary position for a series of 5 or more rounds in a row, we removed all the annotations in the series because the player was likely no longer actually playing the game as designed.
We also removed annotations from the two players cheated by exploiting Javascript vulnerabilities.
Finally, for the recipes genre, a bug during dataset curation resulted in an over-representation of ``all-human'' game rounds played;
for better balance during analysis, we randomly removed a portion of these annotations.
More details on filtering are in Appendix \ref{app:filtering}.
Our final filtered dataset consisted of 21,646 annotations over 7,257 game rounds.
For News, Stories, and Recipes, we had on average over 2 annotations per game round, while for Speeches, a smaller dataset, we had on average 16.
Table \ref{tab:dataset_stats} gives a detailed breakdown of the dataset across genres and generation systems.


\begin{table}[tb]
\center
\small
\begin{tabular}{l|r|rr|r|c|c}
\toprule
 & \# & \multicolumn{2}{|c|}{\# Annotations} & {Avg} & & \\
Genre & Gens & Raw & Final & Ann/Gen & Systems & Decoding Strategies \\
\midrule
News & 1,838 & 7,806 & 4,488 & 2.97 &
\begin{minipage}{.275\textwidth}
      \includegraphics[height=1em]{figures/model_dist_new_york_times}
\end{minipage}
&
\begin{minipage}{.225\textwidth}
      \includegraphics[height=1em]{figures/decoding_dist_new_york_times}
\end{minipage}
\\
Stories & 9,864 & 8,007 & 4,614 & 2.53 &
\begin{minipage}{.275\textwidth}
      \includegraphics[height=1em]{figures/model_dist_short_stories}
\end{minipage}
&
\begin{minipage}{.225\textwidth}
      \includegraphics[height=1em]{figures/decoding_dist_short_stories}
\end{minipage}
\\
Recipes & 7,258 & 17,978 & 7,709 & 2.13 &
\begin{minipage}{.275\textwidth}
      \includegraphics[height=1em]{figures/model_dist_recipes}
\end{minipage}
&
\begin{minipage}{.225\textwidth}
      \includegraphics[height=1em]{figures/decoding_dist_recipes}
\end{minipage}
\\
Speeches & 297 & 8,374 & 4,835 & 16.28 &
\begin{minipage}{.275\textwidth}
      \includegraphics[height=1em]{figures/model_dist_presidential_speeches}
\end{minipage}
&
\begin{minipage}{.225\textwidth}
      \includegraphics[height=1em]{figures/decoding_dist_presidential_speeches}
\end{minipage}
\\
\bottomrule
\end{tabular}
\caption{Statistics on the annotation tasks (game rounds) available in our system. The discrepancies in number of annotations per dataset is partially due to the fact that players were able to choose which domain they performed annotations in.}
\label{tab:dataset_stats}
\end{table}

\begin{table}[tb]
\center
\small
\begin{tabular}{l|rrrrr}
\toprule
Class & \# Participants & \# Annotations & Avg Ann / Part & Avg Score / Part & Avg Time (s)\\
\midrule
Group A & 141 & 6,527 & 46 & 1.966 & {5.651} \\
Group B & 102 & 15,119 & 148 & 2.134 & {6.443} \\
\midrule
Overall & 241 & 21,646 & 90 & 2.083 & {6.338} \\
\bottomrule
\end{tabular}
\caption{Statistics on the students who were invited to complete annotations on \ROFT. ``Avg Ann / Part'' is the average number of annotations per participating student,  while ``Avg Score / Part'' is the average score. ``Avg Time'' is the average time it took a participant to read one sentence. Standard error is shown.
}
\label{tab:particpants}
\end{table}

\subsubsection{Continuation Sources}

In order to answer questions related to how model attributes affect generated text we employed different methods of text generation for each category.
For Recipes, New York Times, and Stories, we generated continuations with GPT-2 XL using nucleus sampling \citep{holtzmanetal2020} with $p=0.4$ and a repetition penalty of $1.2$ \citep{keskar2019ctrl}.
For Recipes, we additionally generated continuations with a GPT-2 XL model finetuned on recipes\footnote{See Appendix \ref{app:gen_filter} for finetuning details.}.

For New York Times and Stories, we experimented with varying the $p$ used for decoding, testing out $p=0.0$ (argmax) and $p=1.0$ (sampling directly from the model's predicted distribution).

We also added 100 attention-check game rounds (10 for each possible context length), where rather than transitioning to machine-generated text, the passage transitioned to sentences from a randomly selected news article.
We refer to this as the ``random'' model in our analysis and expect these game rounds to be trivial for players.

For Stories, we experimented with different model sizes, generating continuations with both GPT-2 Small (117M Parameters) and GPT-2 XL (1.5B Parameters).
Lastly, for Presidential Speeches, we generated continuations using the CTRL model \citep{keskar2019ctrl} rather than GPT-2.
CTRL has the option to specify a control code indicating what domain to generate text in. For half of the generations, we used the ``[Politics]'' control code while for the other half we randomly selected a control code each time.
Table \ref{tab:dataset_stats} gives the statistics of the generations included. Note that not all generations received annotations.



% \TODO{(Checklist) Mention the exit survey explicitly and mention that all players in our data are anonymized and that they consented to their data being used in our research? Also mention the intended use of our dataset. (maybe include that in \S3.6?)}

\subsection{Results}
\label{section:roft_results}
The collected annotations allow us to investigate several questions.
Error bars on all figures and tables are 95\% confidence intervals.
Appendix \ref{app:graph_values} gives the exact values for all bar graphs.

\subsubsection{Can humans detect generated text?}
Players correctly guessed exactly on the boundary sentence 23.4\% of the time.
For game rounds which contained at least one generated sentence, players were able to eventually guess machine-generated 72.3\% of the time, even if they missed the exact boundary. % TODO (Daphne) does this mean predicted >= true && true != 9 ?
Players incorrectly identified 61.3\% of all-human game rounds as containing machine-generated text .

The average number of points (\S\ref{sec:points}) received per round by our players was 2.08, well above random chance.
As baselines, if a player uniform randomly guessed every round, their expected per-round score would be 1.31, and
if they always guessed the last sentence, their expected per-round score would be 1.5.\footnote{These expectations assume that the true boundary position is equally likely to be at any position. Figure \ref{fig:distribution_true_boundaries} shows the true distribution of boundaries, which was not quite uniform.}
For the remaining analyses, we will use average points earned as the primary measure of detection ability.
This measure correlates with other possible metrics (Table \ref{tab:correlations_with_other_metrucs}).

As an additional sanity check on annotator skill, we included game rounds in the News domain where instead of transitioning to an LM-generated continuation, the text transitioned to a completely different news article selected at random.
Out of the 214 annotations we collected for this ``sanity check'' system, the mean score was 2.75, significantly higher than any of the true LM-backed systems.
Also, for these annotations, the error type ``irrelevant'' was selected about twice as often as all other error types combined, validating that players were paying attention to the task at hand.

\subsubsection{How much does player ability vary?}
There was a large variance in the skill of individual players.
Out of the 116 players who completed 50 game rounds, 19 earned a total score of 70 or fewer points (one std below the mean score) in their first 50 rounds, while 15 earned a total score 127 or greater points (one standard deviation above the mean score). Four of these raters scored two standard deviations above the mean score.

We also found that under the right conditions, players can improve over time.
There was no correlation between number of rounds played and player score for Group A.
However, Group B, who were given extra credit proportional to their game score, did show slight improvement (Table \ref{tab:correlation_over_time}). 
Interestingly, there was also lower variance in points earned among Group B (Figure \ref{fig:skill_over_time}).

% We hypothesize that the difference between these two group comes from better guidance as to what to look for in generated text and more performance-based incentives to improve at the task.

% Given the tendency for annotators to game our system by learning to select all sentences as human-written, we hypothesized that this improvement over time may actually be attributable to annotators learning how to maximize points rather than actually improving at the detection task. To combat this, we re-conducted our analysis with much more aggressive filtering and got similar results. A more detailed reporting of this experiment can be found in Appendix \ref{app:gaming}.

We can also measure inter-annotator agreement with the Krippendorff's alpha co-efficient.
This statistic measures how much disagreement there is between players compared to the amount of disagreement one would expect by chance.
Two players are considered to have agreed if they both guessed ``machine-generated'' on any sentence after the true boundary or if they both guessed the entire passage was human-written.
Over all annotations, we found $\alpha$=-0.25, indicating there was less agreement than could be expected from random guessing, suggesting different annotators were better at identifying different kinds of problems with LM-generated text.
However, among our top 25\% of players (measured by mean score), there was high inter-annotator agreement, with  $\alpha$=0.44, suggesting that good annotators made similar errors.

\begin{figure}[tb]
    \centering
    \includegraphics[width=1\linewidth]{figures/domain_topp.pdf}
    \caption{\textbf{(left)} Comparison of mean player score across different genres with GPT-2 XL $p$=0.4 (and CTRL $p$=0.4 for speeches). \textbf{(right)} Comparison of mean player score across different values of $p$ for nucleus sampling (GPT-2 XL), as well as a ``sanity-check'' baseline.}
    \label{fig:genre_top_p}
\end{figure}

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/model_size_and_finetuning.pdf}
    \caption{\textbf{(left)} For Stories, as model size increases (using $p$=0.4), detection becomes harder. \textbf{(middle)} For Recipes, extra finetuning does not significantly impact detectability. \textbf{(right)} For Speeches, using a ``[Politics]'' control has no impact on detectability.}
    \label{fig:model_size_finetuning}
\end{figure}

\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{figures/time_tracking.pdf}
    \caption{\textbf{(left)} Histogram showing the relationship between points earned and the number of seconds an annotation took. \textbf{(right)} Among players who completed at least 100 annotations, average annotation speed decreased with increased experience at the task.
    % We see that high scoring annotations take longer and that players get faster over time.
    % \TODO{nth annotation -> nth game round}
    }
    \label{fig:time_tracking}
\end{figure}


\subsubsection{Are some genres easier to detect?}
We found that generated text was easier to identify in the recipes and speeches genres than in the stories and news genres.
Figure \ref{fig:genre_top_p} (left) shows the average points received on each genre for game rounds that used comparable LMs, while Table \ref{tab:scores_per_domain} gives a more detailed breakdown across models. 

For recipes, we expect that the task was made easier by the fact that the first human-written "sentence" in each game round was a semi-structured ingredients list, making it easy for players to check for contradictions---a step saying to mix in cream is probably generated if there is no cream ingredient.
In addition, recipes often assume implicit unwritten knowledge, which language models struggle to get right---a step saying to crack eggs into a bowl must precede a step saying to  whisk the eggs.
Indeed, if we look at the reasons given by our players for saying ``machine-generated,'' recipes had a much larger percentage of ``common\_sense'' errors (26\%) than did either News (10\%) or Stories (10\%).
It is worth noting that this result slightly contradicts the one reported by \citet{clark2021all} who reported that generated recipes were more difficult to detect than news or stories; more targeted research is necessary to fully understand the relationship between domain and generation performance.

We believe the speech genre was easier for players not because speeches are intrinsically more difficult to generate but because we struggled to get CTRL to produce high-quality, non-repetitive generations, even though it is about the same size model as GPT-2 XL.
It was necessary to incorporate repetition penalties during generation with CTRL, which helped but did not
solve the quality issues.

\subsection{Does model size make a difference?}
Previous work has shown that language model performance scales with number of parameters \citep{kaplan2020scaling}, so we expected players to be worse at detecting generations from larger models.
Indeed, we found that players scored significantly higher when generations came from GPT-2 small (117M parameters) than when they came from GPT-2 XL (1.5B parameters) (Figure \ref{fig:model_size_finetuning}.).

% We additionally experimented with the GPT-3 Davinci model (150B parameters) but were unable to collect enough annotations to draw any statistically significant conclusions.


\subsection{Are diverse generations easier to detect?}
Choice of decoding strategy is known to have significant impact on text quality \citep{zhang2020trading} and detectability \citep{ippolito-etal-2020-automatic}.
Choosing a lower value of $p$ when generating with a nucleus sampling \citep{holtzmanetal2020} decoding strategy produces less diverse but also less noisy text than choosing a higher value of $p$.
In our experiments, we did not find statistically significant differences in player skill between $p$=0.0 (greedy) and $p$=0.4 sampling (Figure \ref{fig:genre_top_p}).
However, players were significantly better at $p$=1 (pure random sampling) than the lower values, 
validating claims from earlier papers that LMs struggle to generate high-quality text with similar diversity to human-written text.
Interestingly, generations from GPT-XL using $p$=1.0 were easier for players to detect than generations from GPT-2 small using $p$=0.4.
This highlights the importance of decoding, as improper selection of decoding strategy may cause a language model to perform worse than one that is one tenth its size.
% This further emphasizes the need for automatic detection tools to be used alongside human players in the boundary detection task as well as the binary detection task.
% maybe mention the differences in reasons given for different p values. mention how the GROVER paper says that human text's perplexity is more similar to p=0.92 (lol)


\subsection{Do control codes affect detectability?}
CTRL is a 1.6B parameter LM trained with controllability in mind.
At inference time, one can pass in a control code, such as ``[Politics]'' or ``[Horror]'' to include the style of the generated text.
We investigated the efficacy of these control codes on the genre of presidential speeches by using ``[Politics]'' for half the generations and randomly selecting control codes for the remaining half.
We found that use of the politics control code did not significantly affect  players' ability to distinguish real from fake text.
This is not to say that control codes do not affect generation; however, it does suggest that the cues used by players to detect generations may not be related to genre-specific details, as least not within the genre of political speeches.
Further work is needed to investigate whether control codes could have influenced detectability in other genres.

\subsection{Does finetuning affect detectability?}
We had expected that finetuning on in-domain text would result in a model that was better able to fool humans.
Counter to expectations, there was a small increase in player detection ability when generations came from GPT-2 finetuned on recipes compared with generations from pre-trained GPT-2. This is despite the fact that the finetuned model had close to half the perplexity of the pre-trained model on a held out test set of 50,000 recipes (4.781 vs. 8.979).
While we can only speculate as to the amount of recipe knowledge present in the pre-trained model (GPT-2's training data is not publicly available), it is possible the pre-trained model already contained enough understanding of recipe-like text that it was not critical to do the extra-finetuning.
Perhaps finetuning would have had more impact in a specialized or jargon-laden domain (e.g. legal, medical).


% However, it may also be the case that a sufficiently long prompt is enough priming to put any vanilla model with broad enough training data into a space where it generates comparative to a finetuned model. Future work should seek to further investigate this area and determine how finetuning may affect both automatic and human detection accuracy.

\begin{figure}[tb]
    \centering
    \includegraphics[scale=0.375]{figures/overtime.pdf}
    \caption{Performance over time for the two player groups (\S\ref{sec:players}). Players in Group B, who were given extra instruction and incentives, improved over time while those in Group A did not.}
    \label{fig:skill_over_time}
\end{figure}

\begin{table}[tb]
\small
\centering
\begin{tabular}{SSSSS} \toprule
    {Group} & {$k$} & {$n$} & {Spearman $\rho$}\\ \midrule
    {A} & {50} & {22} & {-0.03} \\
    {A} & {100} & {13} & {-0.06}  \\
    % {A} & {200} & {4} & {-0.03} \\
    \midrule
    {B} & {50} & {88} & {0.29} \\
    {B} & {100} & {81} & {0.42} \\
    % {B} & {200} & {54} & {0.35} \\
    \bottomrule
\end{tabular}
\caption{The Spearman's rank correlation coefficient between the number of annotations performed before the current annotation and the score on the current annotation, for all $n$ players who have performed $k$ or more annotations. Players in Group B, who were given extra instruction and incentives, improved over time while those in Group A did not.}
\label{tab:correlation_over_time}
\end{table}

\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{figures/survey_results_no_fam.pdf}
    \caption{Violin plots showing results of our mandatory exit survey. A violin plot is a box plot that also provides a density estimation. Results shown are filtered to only include players who did at least 20 rounds. We see that reading the help guide, being a native English speaker, and providing a custom response for your familiarity with NLG all contribute to a higher mean score while high domain expertise does not seem have an affect (except in the case of short stories, where variance is lower for domain experts).}
    \label{fig:survey_results}
\end{figure}

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/ner_stats.pdf}
    \caption{We see that human sentences tended to have a different number of named entities than generated sentences.
    Players picked up on the correct trend in Stories, but not in News or Speeches.}
    \label{fig:sentence_stats}
\end{figure}

\subsection{How much time did game rounds take?}
To understand how much time game rounds took, we logged how many seconds players spent on each sentence decision.
We controlled for instances of players leaving a game open mid-annotation by applying $\min(120, t)$ to all recorded times $t$.
We computed total time per annotation by summing the times for each sentence-level decision.
We found players took longer on annotations where they ended up receiving more points, and players gradually got faster over time (Figure \ref{fig:time_tracking}).
While one might expect longer sentences to take more time to read and make decisions on, we found no correlation between time taken and length of sentence ($\rho$=-0.10), indicating that players take time to think about the task beyond just reading the sentence.

\subsection{What sentence-level features could be used to detect generated text?}
It has been well-studied how generated text differs in basic, measurable ways from human-written text, often due to the choice of decoding strategy.
In particular, we measured how sentence length, part-of-speech distribution, and presence of named entities and novel words differed between the generated and human-written sentences in our dataset, and whether players were able to pick up on these differences.
Figure \ref{fig:sentence_stats} shows the results for named entities, where novel named entities are ones which occured in the current sentence but not in any previous sentences.
We found surprisingly different trends across different genres.
On News and Recipes, the generated sentences tended to have fewer named entities than in human-written sentences.
Annotators did not pick up on these trends, though they may have picked up on the fact that for Stories, the generated sentences tended to have slightly more named entities.

In News and Speeches, machine-generated sentences tended to be shorter than human-written ones, a trend players did not pick up on.
However, for Stories, the generated sentences were on average longer than the human ones, and annotators tended to select longer sentences as the boundary.
Additionally in Stories, generated sentences has on average a greater proportion of adjectives and adverbs, but annotators did not pick up on this trend.


\subsection{Does familiarity affect detectability?}
All participating players filled out an exit survey after completing their annotations. The questions on this survey are in Table \ref{tab:survey_questions}.
Figure \ref{fig:survey_results} shows some of the results.
First, there was not much difference in performance between participants who reported they had never heard of GPT-2/3 and those who reported having considerable familiarity with them.
Interestingly, participants who answered ``other'' and wrote custom responses did end up being better at the task.
Second, participants who admitted that they did not read the help guide tended to perform poorly; all the best players did read the guide.
Third, there was not much difference in ability between native and non-native English speakers.
The very strongest players were not native English speakers.
Finally, we did not observe any correlation between self-reported familiarity with a given genre and detection skill on that genre.
% We see that a larger degree of domain familiarity does not have a statistically significant effect on mean score. However, we see that those who did not read the provided help guide did much worse on average and those that gave a custom response to their familiarity with NLG did much better on average. Finally, while native English speakers did do better on average than non-native speakers, the variance for non-natives was much higher resulting in many of our best players being non-native speakers.

\subsection{What are the most reliable errors to look for when detecting generated text?}
\label{sec:reasons}
Each time a player specified a sentence was machine-generated, they had the option to specify why they made this decision, selecting from a set of pre-defined options (Table \ref{tab:reasons_text}) or else writing down a custom reason.
Table \ref{tab:reasons} shows for each reason, the average number of points earned when that reason was specified.
Like \citet{clark2021all}, we see that conditioning on bad grammar is by far the least reliable way to detect generated text.
In addition, we see that over 30\% of all reasons given for generated text were ``irrelevant,'' a result that stayed consistent across all models and domains.
We note that the three most reliable reasons given (``common\_sense,'' ``irrelevant,'' and ``contradicts\_sentence'') were also the three most common, indicating that improving these attributes will lead to the biggest improvements in generation performance.
A more detailed breakdown, as well as several examples of custom reasons, can be found in Appendix \ref{app:reasons}.

\begin{table}[tb]
\small
\center
\begin{tabular}{lcc} \toprule
    {Reason} & {$n$} & {Mean Score}\\ \midrule
    {common\_sense} & {2,432} & {2.566 $\pm$ 0.086} \\
    {irrelevant} & {4,259} & {2.530 $\pm$ 0.064}  \\
    {contradicts\_sentence} & {1,606} & {2.527 $\pm$ 0.105}  \\ 
    {contradicts\_knowledge} & {1,411} & {2.262 $\pm$ 0.111}   \\
    {coreference} & {542} & {2.249 $\pm$ 0.176} \\
    {repetition} & {728} & {2.128 $\pm$ 0.154} \\
    {other} & {75} & {2.040 $\pm$ 0.483} \\
    {generic} & {1,546} & {1.920 $\pm$ 0.101} \\
    {grammar} & {1,539} & {1.780 $\pm$ 0.105} \\\bottomrule
\end{tabular}
\caption{
The number of times each reason was given for text being machine-generated, and the mean score over those annotations.
We see that when players select reasons like ``grammar'' or ``generic,'' they are much less likely to be correct than when selecting ``common\_sense'' or ``irrelevant.''}
\label{tab:reasons}
\end{table}

\subsection{Discussion}
\label{section:roft_discussion}
In this section, we have demonstrated the viability of the boundary detection task as a framework for soliciting human evaluation of natural-language generation systems.
We conducted the largest study of generated text detectability to date and, in the process, replicated many previous major results in the field, such as the improved performance of bigger models \citep{kaplan2020scaling}, the importance of decoding strategy selection \citep{ippolito-etal-2020-automatic}, and the difficulty in incentivizing annotators to improve over time \citep{clark2021all}.
In addition, we have provided new insights into the ways in which humans interact with partially-generated text.

In terms of ethics, work on the detectability of machine-generated text sits at an interesting balancing point.
On one hand, gamifying and publicizing the detection task may help to raise the public's awareness of their susceptibility to machine-generated text, and work such as ours paves the way for future research on techniques for helping the public to improve at detection.
On the other hand, we show that the detection task is a viable method for evaluating generation systems.
For researchers aiming to build better generative language models, decreasing human detection ability might a very reasonable goal to optimize for.
As much as our project seeks to better understand and improve human detection, our results can just as easily be used to make generative models even less detectable than they already are.
Despite this drawback, we nonetheless believe it is important to study detection as a means of assessing the risks that language models pose and protecting against future harm.

One major area of improvement in our study is to develop better incentives for players to give good annotations that are unbiased by system design.
Many of the students given extra credit proportional to the amount of points they scored learned they could exploit the point system by always picking one of the later sentences as the boundary.
They found that rapidly guessing Sentence 9 as the boundary on every game round was a more effective strategy for maximizing earned points per time spent than taking the time to carefully read the text in each round.
One alternative system which could reduce this bias would be to show all ten sentences in the passage at once rather than show them one at a time.
The player would get a certain number of tries to guess the index of the boundary sentence and would be scored based on the number of tries this takes.
This would resolve the bug in our current system that some sentence positions have a high point value in expectation.

In addition to the game design aspect of the study, there are many more areas for future work to investigate. To start, our study tested only a limited subset of models and domains. It would be interesting to see if larger models, such as GPT-3 \citep{brownetal2020}, continue to show similar results across the variables we investigated (finetuning, topic control, decoding strategy). Another area of improvement is in the nature of the boundaries. Our study was limited in that we assumed continuations always happened on the sentence boundary, but that is not always necessarily the case. Future work should look at continuations that do not happen exactly on the boundary between sentences. While this would require major edits to the detection task, interface, and incentive structure, it would allow for a much more realistic representation of what generated text looks like in the real world if done properly. 

Finally, we believe that future work should develop techniques to betetr investigate what exactly annotators are thinking when they make their decisions. Researchers need to help give annotators the language to properly explain how and why they come to select the sentences that they do, whether this be in the form of better error categorization or in free text responses. In addition, future work should seek to draw from a more diverse pool of annotators. We acknowledge that university students (many of whom have studied computer science) may not be representative of the larger population and we hope that future work can take on a larger user study on a more diverse population and investigate how the unique backgrounds of different annotators contribute to how they detect generated text.


\subsection{Summary of Contributions}
The RoFT platofmr was introduced as a system demonstration at the 2020 Conference on Empirical Methods in Natural Language Processing \citep{dugan2020roft}.
The RoFT website was implemented by Arun Kirubarajan, Liam Dugan, and myself, with the assistance of Run Shi, and the mentorship or Chris Callison-Burch.
The user study was designed and run, and its results analyzed, by Liam and myself.